<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://iamalinguist.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iamalinguist.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-17T18:55:44+00:00</updated><id>https://iamalinguist.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Argument Structure</title><link href="https://iamalinguist.github.io/blog/2025/argument-structure/" rel="alternate" type="text/html" title="Argument Structure"/><published>2025-09-13T00:00:00+00:00</published><updated>2025-09-13T00:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/argument-structure</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/argument-structure/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Information Structure</title><link href="https://iamalinguist.github.io/blog/2025/information-structure/" rel="alternate" type="text/html" title="Information Structure"/><published>2025-09-13T00:00:00+00:00</published><updated>2025-09-13T00:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/information-structure</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/information-structure/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">understanding Anaphors, Personal Pronouns, and R-Expressions in Hindi</title><link href="https://iamalinguist.github.io/blog/2025/understanding-pronouns/" rel="alternate" type="text/html" title="understanding Anaphors, Personal Pronouns, and R-Expressions in Hindi"/><published>2025-07-24T00:00:00+00:00</published><updated>2025-07-24T00:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/understanding-pronouns</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/understanding-pronouns/"><![CDATA[<p>In the field of syntax and semantics, particularly within the framework of generative grammar, three important categories of noun phrases often arise: <strong>anaphors</strong>, <strong>personal pronouns</strong>, and <strong>R-expressions</strong>. While these concepts are universal in natural languages, their behavior can differ significantly across languages. In this article, we’ll explore how these categories operate in <strong>Hindi</strong>, a South Asian language rich in syntactic variety and nuance.</p> <hr/> <h2 id="basic-definitions-cross-linguistic">Basic Definitions (Cross-Linguistic)</h2> <p>Before we dive into Hindi specifically, let’s clarify the three categories:</p> <ul> <li><strong>Anaphors</strong>: These are expressions that must refer to another expression (antecedent) within a local domain. In English, “himself” or “herself” are examples.</li> <li><strong>Personal Pronouns</strong>: These can refer to other entities in discourse, but crucially, they <strong>cannot</strong> be co-referential with a noun phrase in their local domain (e.g., “He_i loves him_i” is not allowed).</li> <li><strong>R-expressions (Referential expressions)</strong>: These are full noun phrases like “Ram” or “the boy” that refer independently and <strong>must not</strong> be co-referential with a c-commanding NP in the same clause.</li> </ul> <p>These categories follow the <strong>Binding Theory</strong>, particularly formulated in Principles A, B, and C.</p> <hr/> <h2 id="these-categories-in-hindi">These Categories in Hindi</h2> <p>Hindi exhibits interesting behaviors for each of these categories. Let’s explore them one by one.</p> <hr/> <h3 id="1-anaphors-in-hindi-आत्मनिर्भर-सर्वनाम">1. Anaphors in Hindi (आत्मनिर्भर सर्वनाम)</h3> <p>In Hindi, the primary anaphor is:</p> <ul> <li><strong>“अपने (apne)”</strong>: This reflexive possessive pronoun is commonly used and must refer to a subject within a local clause.</li> </ul> <h4 id="examples">Examples:</h4> <ol> <li> <p><strong>राम ने अपने माता-पिता को बुलाया।</strong><br/> <em>Rām ne apne mātā-pitā ko bulāyā.</em><br/> “Ram called his (own) parents.”<br/> ✅ <em>‘Apne’ refers to Ram – grammatical.</em></p> </li> <li> <p><strong>सीता कहती है कि राम ने अपने माता-पिता को बुलाया।</strong><br/> <em>Sītā kahtī hai ki Rām ne apne mātā-pitā ko bulāyā.</em><br/> “Sita says that Ram called his (own) parents.”<br/> ✅ <em>‘Apne’ refers to Ram, not Sita – binding allowed within embedded clause.</em></p> </li> <li> <p><strong>सीता ने कहा कि अपने माता-पिता को राम ने बुलाया।</strong><br/> <em>Sītā ne kahā ki apne mātā-pitā ko Rām ne bulāyā.</em><br/> ❌ <em>‘Apne’ cannot refer to Sita here because the local subject is Ram.</em></p> </li> </ol> <blockquote> <p>🧠 <strong>Key insight</strong>: In Hindi, “apne” behaves as an anaphor and must be <strong>locally bound</strong>, typically within the same clause. This is consistent with <strong>Binding Principle A</strong>.</p> </blockquote> <hr/> <h3 id="2-personal-pronouns-in-hindi-पुरुषवाचक-सर्वनाम">2. Personal Pronouns in Hindi (पुरुषवाचक सर्वनाम)</h3> <p>Hindi uses pronouns like:</p> <ul> <li><strong>“वह (vah)”, “उसने (usne)”, “उसको (usko)”</strong> – he/him/she/her depending on case.</li> <li><strong>“वे (ve)”, “उन्होंने (unhoṇe)”</strong> – plural forms.</li> </ul> <p>These cannot refer back to a local subject within the same clause.</p> <h4 id="example">Example:</h4> <ol> <li> <p><strong>राम_i ने कहा कि वह_i बीमार है।</strong><br/> <em>Rām ne kahā ki vah bīmār hai.</em><br/> “Ram said that he is sick.”<br/> ✅ <em>‘Vah’ can refer to Ram, as it’s outside the local clause.</em></p> </li> <li> <p><strong>राम_i ने उसे_i मारा।</strong><br/> <em>Rām ne use mārā.</em><br/> “Ram hit him.”<br/> ❌ <em>Not allowed if ‘use’ refers to Ram himself – violates Principle B.</em></p> </li> </ol> <blockquote> <p>🧠 <strong>Key insight</strong>: Hindi personal pronouns follow <strong>Binding Principle B</strong>, meaning they must be <strong>free</strong> (not bound) within their local domain.</p> </blockquote> <hr/> <h3 id="3-r-expressions-in-hindi-स्वतंत्र-संज्ञाएँ">3. R-Expressions in Hindi (स्वतंत्र संज्ञाएँ)</h3> <p>R-expressions in Hindi are proper nouns or definite noun phrases such as:</p> <ul> <li><strong>राम (Ram), सीता (Sita), वह लड़का (that boy)</strong></li> </ul> <p>These expressions cannot be coreferential with another NP that c-commands them in the same clause.</p> <h4 id="example-1">Example:</h4> <ol> <li> <p><strong>उसने_i राम_i को मारा।</strong><br/> <em>Usne Rām ko mārā.</em><br/> “He hit Ram.”<br/> ❌ <em>Not allowed if ‘usne’ = ‘Ram’ — violates Principle C.</em></p> </li> <li> <p><strong>राम_i ने कहा कि सीता राम_i को जानती है।</strong><br/> <em>Rām ne kahā ki Sītā Rām ko jāntī hai.</em><br/> ✅ <em>Allowed, because ‘Ram’ is not c-commanded by the co-referent in the embedded clause.</em></p> </li> </ol> <blockquote> <p>🧠 <strong>Key insight</strong>: R-expressions must be <strong>free everywhere</strong>, as per <strong>Binding Principle C</strong>.</p> </blockquote> <hr/> <h3 id="summary-table-">Summary Table 📚</h3> <table> <thead> <tr> <th>Category</th> <th>Hindi Example</th> <th>Binding Principle</th> <th>Coreference Allowed?</th> </tr> </thead> <tbody> <tr> <td><strong>Anaphor</strong></td> <td>अपने (apne)</td> <td>Principle A</td> <td>✅ Within local domain</td> </tr> <tr> <td><strong>Pronoun</strong></td> <td>वह, उसे, उसने (vah, usko, usne)</td> <td>Principle B</td> <td>❌ Within local domain</td> </tr> <tr> <td><strong>R-Expression</strong></td> <td>राम, सीता (Ram, Sita)</td> <td>Principle C</td> <td>❌ If c-commanded in same clause</td> </tr> </tbody> </table> <hr/> <h3 id="why-is-this-important-">Why Is This Important? 🧐</h3> <p>Understanding how Hindi handles binding is not only essential for syntactic theory but also for applications like:</p> <ul> <li><strong>Natural Language Processing (NLP)</strong> and machine translation</li> <li><strong>Second language acquisition</strong></li> <li><strong>Cross-linguistic comparison</strong> in typology</li> <li><strong>Psycholinguistics</strong> and sentence processing studies</li> </ul> <p>Hindi, like many Indo-Aryan languages, provides a rich testing ground for theories of binding and co-reference. Its anaphoric system (especially with “apne”) showcases locality constraints vividly. Meanwhile, personal pronouns and R-expressions align well with the universals proposed in Binding Theory.</p>]]></content><author><name></name></author><category term="hi-linguistics"/><category term="theo-linguistics"/><category term="explanations"/><category term="r"/><category term="expression"/><category term="personal"/><category term="pronoun"/><category term="anaphors"/><summary type="html"><![CDATA[Understanding Anaphors, Personal Pronouns, and R-Expressions in Hindi]]></summary></entry><entry><title type="html">understanding hpsg</title><link href="https://iamalinguist.github.io/blog/2025/understanding-hpsg/" rel="alternate" type="text/html" title="understanding hpsg"/><published>2025-06-14T00:00:00+00:00</published><updated>2025-06-14T00:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/understanding-hpsg</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/understanding-hpsg/"><![CDATA[<p>HPSG combines ideas from:</p> <ul> <li>Unification-based grammar</li> <li>Lexicalist theories</li> <li>Constraint-based syntax and semantics</li> </ul> <p>It’s known for using <em>typed feature structures</em> and modeling grammar in a way that <em>integrates syntax, semantics, and the lexicon</em> in a highly modular and formal way. Let us see a quick visual schema of how HPSG organizes sentence structure-</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">
</span>[ S ]
 |-- HEAD: verb
 |-- SUBJ: &lt; NP &gt;
 |-- COMPS: &lt; NP &gt;
 |-- SYNSEM:
 |     |-- LOCAL:
 |     |     |-- CAT: [HEAD verb, SUBJ &lt;NP&gt;, COMPS &lt;NP&gt;]
 |     |     |-- CONTENT: [Semantic Representation]
 |
 |-- DTRS (Daughters):
       |
       |-- [ NP ]  ← Subject (e.g., "John")
       |
       |-- [ V' ]
             |
             |-- HEAD: verb
             |-- COMPS: &lt; NP &gt;
             |-- DTRS:
                   |-- [ V ]      ← Verb (e.g., "gave")
                   |-- [ NP ]     ← Object (e.g., "a book")
<span class="p">```</span>
</code></pre></div></div> <hr/> <h3 id="1--s---sentence-node">1. <code class="language-plaintext highlighter-rouge">[ S ]</code> – Sentence node</h3> <ul> <li>The top-level phrase (S for Sentence).</li> <li>A <strong>headed phrase</strong>, meaning the <strong>verb</strong> determines the grammatical behavior.</li> </ul> <hr/> <h3 id="2-head-verb">2. <code class="language-plaintext highlighter-rouge">HEAD: verb</code></h3> <ul> <li>The <strong>head</strong> of the sentence is the <strong>verb</strong> (e.g., “gave”).</li> <li>The head determines the phrase’s syntactic and semantic properties.</li> </ul> <hr/> <h3 id="3-subj-np">3. <code class="language-plaintext highlighter-rouge">SUBJ: &lt;NP&gt;</code></h3> <ul> <li>Specifies the <strong>subject</strong> requirement of the verb (like “John”).</li> <li>It is a list of NPs that the verb expects in the <strong>SUBJ</strong> feature.</li> </ul> <hr/> <h3 id="4-comps-np">4. <code class="language-plaintext highlighter-rouge">COMPS: &lt;NP&gt;</code></h3> <ul> <li>Specifies the <strong>complements</strong> required by the verb (like objects).</li> <li>“Gave” requires two NPs (e.g., “a book” and “to Mary”).</li> </ul> <hr/> <h3 id="5-synsem-syntactic-and-semantic-features">5. <code class="language-plaintext highlighter-rouge">SYNSEM</code> (Syntactic and Semantic Features)</h3> <ul> <li>Merges <strong>syntax and semantics</strong> into a unified structure.</li> <li> <p>Contains:</p> <ul> <li><strong>LOCAL</strong>: <ul> <li><code class="language-plaintext highlighter-rouge">CAT</code>: category features (HEAD, SUBJ, COMPS)</li> <li><code class="language-plaintext highlighter-rouge">CONTENT</code>: semantic representation (meaning of the phrase)</li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="6-dtrs-daughters">6. <code class="language-plaintext highlighter-rouge">DTRS</code> (Daughters)</h3> <ul> <li> <p>Immediate components of the phrase:</p> <ul> <li><code class="language-plaintext highlighter-rouge">NP</code> – the <strong>subject</strong> (“John”)</li> <li><code class="language-plaintext highlighter-rouge">V'</code> – the <strong>verb phrase</strong>, which contains: <ul> <li><code class="language-plaintext highlighter-rouge">V</code> – the verb (“gave”)</li> <li><code class="language-plaintext highlighter-rouge">NP</code> – the object (“a book”)</li> </ul> </li> </ul> </li> <li> <p>Follows the <strong>Head Feature Principle</strong> (explained below).</p> </li> </ul> <hr/> <p><br/> <br/></p> <h4 id="-key-principles-in-the-schema">✅ Key Principles in the Schema</h4> <table> <thead> <tr> <th>Principle</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><strong>Head Feature Principle</strong></td> <td>The head daughter passes up its features to the mother node.</td> </tr> <tr> <td><strong>Valence Principle</strong></td> <td>SUBJ and COMPS lists are discharged as syntactic elements combine.</td> </tr> <tr> <td><strong>Feature Structures</strong></td> <td>Every node is an Attribute-Value Matrix (AVM).</td> </tr> <tr> <td><strong>Lexicalism</strong></td> <td>Most grammatical information is stored in the lexicon. For example, the verb “gave” encodes that it takes a subject and two complements.</td> </tr> </tbody> </table> <hr/> <p><br/> <br/></p> <h4 id="hpsg-tree-diagram-with-avms-for-john-gave-a-book">HPSG Tree Diagram with AVMs for “John gave a book”</h4> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">
</span>                                 [S]
                +----------------+----------------+
                |                                 |
           [NP] John                      [V'] (Verb Phrase)
                                              |
                                +-------------+-------------+
                                |                           |
                         [V] "gave"                   [NP] "a book"

AVMs for Each Node:

[S]
⎡HEAD   verb             ⎤
⎢SUBJ   ⟨NP⟩              ⎥
⎢COMPS  ⟨NP⟩              ⎥
⎢CONT   give'(x, y)      ⎥
⎣                        ⎦

[NP] → "John"
⎡HEAD   noun             ⎤
⎢INDEX  j                ⎥
⎢CONT   john'            ⎥
⎣                        ⎦

[V']
⎡HEAD   verb             ⎤
⎢COMPS  ⟨NP⟩              ⎥
⎢CONT   λx. give'(j, x)  ⎥
⎣                        ⎦

[V] → "gave"
⎡HEAD   verb             ⎤
⎢SUBJ   ⟨NP⟩              ⎥
⎢COMPS  ⟨NP⟩              ⎥
⎢CONT   λxλy. give'(y, x)⎥
⎣                        ⎦

[NP] → "a book"
⎡HEAD   noun             ⎤
⎢INDEX  b                ⎥
⎢CONT   book'(b)         ⎥
⎣                        ⎦

<span class="p">```</span>
</code></pre></div></div> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>The verb “gave” expects a subject (SUBJ ⟨NP⟩) and a complement (COMPS ⟨NP⟩), both noun phrases.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>The semantics (CONT) is built through lambda abstraction.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>“John” is the subject, indexed as j, and “a book” is the object, indexed as b.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>The semantic content of the sentence becomes give’(john’, book’).</li> </ul> <hr/> <p><br/> <br/></p> <h2 id="definitions">Definitions</h2> <h4 id="1-unification-based-grammar">1. Unification-based Grammar</h4> <p>🔍 Idea: Grammar rules operate by matching and merging feature structures (attribute-value pairs). This merging is called unification. <br/> 🧠 Analogy: Think of two puzzle pieces (feature structures). You can join them (unify) if they match—but if there’s a mismatch (e.g., one says “singular”, the other says “plural”), unification fails. <br/> 📝 Example: Let’s look at subject-verb agreement: “She walks.” ✅ || “She walk.” ❌</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">
</span>[NP]
⎡ PER 3rd ⎤
⎢ NUM sg  ⎥
⎣ GEN f   ⎦

[V]
⎡ HEAD verb ⎤
⎢ AGR ⎡ PER 3rd ⎤
⎢     ⎢ NUM sg  ⎥
⎣     ⎣ GEN f   ⎦
<span class="p">```</span>
</code></pre></div></div> <p>✅ These unify! So the sentence is grammatical. In “She walk”, the verb’s AGR is plural (say: NUM pl), which fails to unify with the subject (NUM sg). So the structure is rejected.</p> <hr/> <p><br/></p> <h4 id="2-lexicalist-theories">2. Lexicalist Theories</h4> <p>🔍 Idea: Lexicon is rich. Words carry most of the syntactic and semantic information. Instead of complex grammar rules, each word specifies what it needs (like arguments, tense, etc.). <br/> 📝 Example: For the verb “give”:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">
</span>Lexical Entry for "give":

⎡ HEAD   verb                   ⎤
⎢ SUBJ   &lt;NP&gt;                   ⎥
⎢ COMPS  &lt;NP, NP&gt;               ⎥
⎢ SEM    λxλyλz. give'(z, y, x) ⎥
⎣                               ⎦
<span class="p">```</span>
</code></pre></div></div> <p>“give” requires: <em>(1) subject (e.g., “John”)</em> and (2) <em>complements (e.g., “a book”, “to Mary”)</em>. So:</p> <ul> <li>“John gave a book to Mary” ✅ (all arguments present)</li> <li>“John gave” ❌ (incomplete)</li> </ul> <p>In HPSG, this rich lexical information drives the sentence structure, hence: Lexicalist.</p> <hr/> <p><br/></p> <h4 id="3-constraint-based-syntax-and-semantics">3. Constraint-based Syntax and Semantics</h4> <p>🔍 Idea: Rather than transformational rules (like in Chomsky’s theories), HPSG uses constraints that must be satisfied. These constraints apply to feature structures. There are no derivations, only well-formedness constraints on structure.</p> <p>📝 Example: You want to form a head-complement phrase like: “read a book”</p> <ul> <li>The phrase must satisfy constraints like: <ul> <li>The verb (head) must specify it takes one NP as complement.</li> <li>The complement NP must match what the verb expects (e.g., a proper noun phrase).</li> <li>The semantic roles must link correctly: read’(x, y) where x is the subject, y is the object.</li> </ul> </li> </ul> <p>In HPSG, a Head-Complement Schema might look like:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">
</span>[SYN] ⎡ HEAD verb ⎤
      ⎢ COMPS &lt; &gt; ⎥ ← Empty after all complements satisfied
      ⎣           ⎦

DTRs (daughters):
- Head Daughter: [HEAD verb, COMPS &lt;NP&gt;]
- Non-head Daughter: [NP]
<span class="p">```</span>
</code></pre></div></div> <p><br/> The constraint says: “You can combine a verb phrase with an NP if that NP satisfies the verb’s COMPS list”. This is declarative (constraint-based), not procedural.</p> <p><br/> <br/></p> <h2 id="summary-table">Summary Table</h2> <hr/> <table> <thead> <tr> <th>Concept</th> <th>What it Means</th> <th>HPSG’s Implementation</th> </tr> </thead> <tbody> <tr> <td>Unification-based Grammar</td> <td>Merge of feature structures</td> <td>Grammar rules succeed only if features match</td> </tr> <tr> <td>Lexicalist Theories</td> <td>Lexicon is rich with syntactic/semantic info</td> <td>Words define argument structure, not grammar rules</td> </tr> <tr> <td>Constraint-based Syntax/Sem</td> <td>Well-formedness defined via feature constraints</td> <td>Syntactic rules are constraints, not transformations</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="theo-linguistics"/><category term="explanations"/><category term="grammar"/><category term="syntax"/><category term="unification"/><category term="lexicalist"/><category term="constraint"/><summary type="html"><![CDATA[A quick visual schema of 'How HPSG organizes sentence structure?']]></summary></entry><entry><title type="html">understanding tgg</title><link href="https://iamalinguist.github.io/blog/2025/understanding-tgg/" rel="alternate" type="text/html" title="understanding tgg"/><published>2025-06-14T00:00:00+00:00</published><updated>2025-06-14T00:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/understanding-tgg</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/understanding-tgg/"><![CDATA[<h2 id="what-is-tgg">What is TGG?</h2> <ul> <li> <p>Transformational Generative Grammar(TGG) is a specific type of <a href="2025-06-14-generative-grammar.md">generative grammar</a>. Introduced by Chomsky in his book “Syntactic Structures” (1957). <br/></p> </li> <li> <p>It introduces transformations — operations that convert one syntactic structure into another. For example, converting: “John is eating an apple.” → “Is John eating an apple?” (yes-no question) <br/></p> </li> <li> <p>It makes use of “phrase structures”. When linguists talk about phrase structure, they are referring to the hierarchical organization of words into larger units (phrases) within a sentence. Thus, this concept provides use of alternative names to this viz. Phrase Structure Grammar (PSG), also known as Constituency Grammar.</p> </li> </ul> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">
</span>Which other grammatical theory is it affiliated with? -&gt; **X-bar theory** (A refinement of phrase structure rules that introduced intermediate phrase levels (e.g., X′ or X-bar), ensuring a uniform structure across all phrases (NPs, VPs, etc.).), **Minimalist Program** (While it reduces reliance on phrase structure rules, it still assumes hierarchical phrase structure via Merge operations.)
<span class="p">```</span>
</code></pre></div></div> <ul> <li>Chomsky introduced deep structure (basic sentence structure) and surface structure (what we actually say/hear), both built on phrase structure in TGG. Chomsky talked about these structurs especially in the <strong>Standard Theory (1965)</strong>, while discussing TGG.</li> </ul> <p>🌱 <strong>What are the differences between Deep and Surface Structure?</strong></p> <table> <thead> <tr> <th>Term</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>Surface Structure</td> <td>The <em>final, actual form</em> of a sentence that we <em>speak or hear</em>, after transformations are applied.</td> </tr> <tr> <td>Deep Structure</td> <td>The <em>abstract, underlying form</em> of a sentence that captures its <em>core meaning</em> and <em>basic grammatical relations</em>.</td> </tr> </tbody> </table> <p><br/></p> <p>🌱 <strong>Why Two Levels?</strong></p> <ul> <li>Some sentences may have <strong>ambiguous surface forms</strong> that deep structure helps clarify.</li> <li>Different <strong>surface forms</strong> can express the <strong>same underlying meaning</strong>.</li> </ul> <p><br/></p> <p>🌱 <strong>Examples</strong></p> <p>A. English Examples</p> <h4 id="1-active-and-passive-voice">1. Active and Passive Voice</h4> <ul> <li><strong>Active</strong>: <em>The cat chased the mouse.</em></li> <li><strong>Passive</strong>: <em>The mouse was chased by the cat.</em></li> </ul> <blockquote> <p>✅ <strong>Same Deep Structure</strong>: “cat chases mouse”<br/> ✅ <strong>Different Surface Structures</strong> due to transformation.</p> </blockquote> <p><br/></p> <h4 id="2-question-formation">2. Question Formation</h4> <ul> <li><strong>Statement</strong>: <em>You are eating an apple.</em></li> <li><strong>Question</strong>: <em>Are you eating an apple?</em></li> </ul> <blockquote> <p>✅ Deep structure stays the same.<br/> ✅ Surface structure changes through auxiliary movement.</p> </blockquote> <hr/> <p>B. Hindi Examples</p> <h4 id="1-active-passive">1. Active-Passive</h4> <ul> <li><strong>Active</strong>: <em>राम ने सीता को देखा।</em></li> <li><strong>Passive</strong>: <em>सीता राम द्वारा देखी गई।</em></li> </ul> <blockquote> <p>✅ Deep structure: Ram sees Sita<br/> ✅ Surface structure: Changes with voice</p> </blockquote> <p><br/></p> <h4 id="2-question-formation-1">2. Question Formation</h4> <ul> <li><strong>Statement</strong>: <em>तुम बाज़ार जा रहे हो।</em></li> <li><strong>Question</strong>: <em>क्या तुम बाज़ार जा रहे हो?</em></li> </ul> <blockquote> <p>✅ Deep structure = “You are going to the market”<br/> ✅ Surface structure = Adds “क्या” to form question</p> </blockquote>]]></content><author><name></name></author><category term="hi-linguistics"/><category term="theo-linguistics"/><category term="explanations"/><category term="tgg"/><category term="deep"/><category term="surface"/><category term="x-bar"/><summary type="html"><![CDATA[A quick lesson on phrase structures and transformational in a language]]></summary></entry><entry><title type="html">generative grammar- what it actually is?</title><link href="https://iamalinguist.github.io/blog/2025/generative-grammar/" rel="alternate" type="text/html" title="generative grammar- what it actually is?"/><published>2025-06-13T00:00:00+00:00</published><updated>2025-06-13T00:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/generative-grammar</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/generative-grammar/"><![CDATA[<h3 id="generative-grammar-umbrella-term">Generative Grammar (Umbrella Term)</h3> <ul> <li> <p>“Generative Grammar” is a broad theoretical framework in linguistics.</p> </li> <li> <p>It refers to any theory that aims to explicitly generate all and only the grammatical sentences of a language.</p> </li> <li> <p>Introduced by Noam Chomsky in the 1950s.</p> </li> <li> <p>It includes various specific models or versions developed over time. It includes-</p> <ul> <li><a href="../_posts/2025-06-14-understanding-tgg.md">Transformational Generative Grammar (TGG)</a> <ul> <li>Transformational (Generative) Grammar is the most famous and historically important grammar system. This is essentially just the full name for Transformational Grammar (TG).</li> </ul> </li> <li>Generalized Phrase Structure Grammar (GPSG)</li> <li><a href="2025-06-14-understanding-hpsg.md">Head-driven Phrase Structure Grammar (HPSG)</a></li> <li>Lexical Functional Grammar (LFG)</li> <li>Categorial Grammar (CG)</li> <li>Minimalist Program (MP)</li> </ul> </li> </ul> <hr/>]]></content><author><name></name></author><category term="theo-linguistics"/><category term="explanations"/><category term="generative"/><category term="grammar"/><category term="tgg"/><category term="gpsg"/><category term="hpsg"/><category term="lfg"/><category term="cg"/><category term="mp"/><summary type="html"><![CDATA[A quick discussion on the broader term 'generative grammar']]></summary></entry><entry><title type="html">an overview of the grammatical theories</title><link href="https://iamalinguist.github.io/blog/2025/grammatical-theories/" rel="alternate" type="text/html" title="an overview of the grammatical theories"/><published>2025-06-07T21:01:00+00:00</published><updated>2025-06-07T21:01:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/grammatical-theories</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/grammatical-theories/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/chronology-480.webp 480w,/assets/img/chronology-800.webp 800w,/assets/img/chronology-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/chronology.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Timeline of Major Grammar Theories: Traditional Grammar (TG), Structural Grammar (SG), Categorial Grammar (CG), Transformational-Generative Grammar (TGG), Dependency Grammar (DG), Functional Grammar (FG), Lexical Functional Grammar (LFG), Construction Grammar (CG), and Head-Driven Phrase Structure Grammar (HPSG). </div> <p><br/> <br/> <br/></p> <h2 id="1-traditional-grammar-tg">1. Traditional Grammar (TG)</h2> <ul> <li>Period of Dominance: Predominantly before the 20th century, though its influence persists.</li> <li>Core Idea: Rooted in ancient Greek and Latin grammar, its principles have been influential for centuries, dating back to classical antiquity (e.g., Pāṇini in 6th-5th century BC India, Dionysius Thrax in 2nd century BC Greece). It focuses on prescribing “correct” usage based on established rules, often viewing Latin as the ideal model for all languages.</li> <li>Key Features: <ul> <li>Prescriptive: Dictates how language should be used, rather than describing how it is used.</li> <li>Categorization: Emphasizes parts of speech (noun, verb, adjective, etc.) and their morphological forms (e.g., conjugations, declensions).</li> <li>Syntactic Analysis: Primarily sentence-level analysis, often using parsing methods to identify subject, predicate, object, etc.</li> <li>Focus on Meaning: Relies heavily on semantic notions (e.g., “a noun is the name of a person, place, or thing”).</li> <li>Limitations: Often struggles with the diversity and fluidity of natural language, sometimes imposing rules that don’t reflect actual usage.</li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="2-structural-grammar-sg">2. Structural Grammar (SG)</h2> <ul> <li>Period of Dominance: Mid-20th century (roughly 1930s-1960s).</li> <li>Core Idea: Developed as a reaction against the prescriptivism and semantic basis of Traditional Grammar. It advocates for the scientific study of language by focusing on observable linguistic data (utterances) and their distribution. Influenced by Bloomfieldian linguistics.</li> <li>Key Features: <ul> <li>Descriptive: Aims to describe how language is used, without imposing external norms.</li> <li>Empirical: Relies on analyzing actual language data.</li> <li>Distributional Analysis: Identifies linguistic units (phonemes, morphemes, words) based on where and how they occur in relation to other units.</li> <li>Immediate Constituent Analysis (ICA): A method for breaking down sentences into their immediate structural components, often represented by tree diagrams.</li> <li>Form over Meaning: Prioritizes the formal arrangement of linguistic elements over their meaning in analysis.</li> <li>Limitations: Often criticized for not adequately accounting for the creativity of language or the speaker’s underlying knowledge.</li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="3-categorial-grammar-cg">3. Categorial Grammar (CG)</h2> <ul> <li>Core Idea: Views grammar as a system of types, where words are assigned categories that specify their combinatory properties. It is a highly formal and mathematically oriented approach.</li> <li>Key Features: <ul> <li>Functional Application: The primary mechanism for combining linguistic units. Categories are often represented as “functions” that take arguments and produce new categories.</li> <li>Lexicalized: All syntactic information is encoded in the lexical entries of words.</li> <li>Directionality: Categories specify the direction in which they combine with other categories (e.g., a verb might take a noun phrase to its right).</li> <li>No Phrase Structure Rules: Syntactic structure emerges directly from the combination of lexical categories.</li> <li>Semantic Transparency: Often aims for a direct mapping between syntactic structure and semantic interpretation.</li> <li>Varieties: Different systems exist, such as the Lambek Calculus, which has strong connections to mathematical logic.</li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="4-transformational-generative-grammar-tgg">4. Transformational-Generative Grammar (TGG)</h2> <ul> <li>Period of Dominance: From the late 1950s onwards, heavily associated with Noam Chomsky.</li> <li>Core Idea: A significant paradigm shift that proposed that linguistic knowledge is innate and that speakers possess a “generative” grammar that allows them to produce and understand an infinite number of sentences. It distinguishes between deep structure (the underlying meaning) and surface structure (the actual spoken or written form).</li> <li>Key Features: <ul> <li>Generative: Aims to provide a set of explicit rules that can generate all and only the grammatical sentences of a language.</li> <li>Competence vs. Performance: Distinguishes between a speaker’s underlying linguistic knowledge (competence) and their actual use of language (performance).</li> <li>Deep Structure &amp; Surface Structure: Sentences have an abstract deep structure (representing core meaning and grammatical relations) and a surface structure (how they are pronounced). Transformations map deep structures to surface structures.</li> <li>Universal Grammar (UG): Proposes that humans are born with an innate linguistic faculty that provides universal principles common to all languages.</li> <li>Recursion: Explains how language can generate infinitely long sentences through repetitive application of rules.</li> <li>Evolution: Has undergone several revisions (e.g., Standard Theory, Extended Standard Theory, Government and Binding Theory, Minimalist Program).</li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="5-dependency-grammar-dg">5. Dependency Grammar (DG)</h2> <ul> <li>Core Idea: Focuses on dependency relations between words in a sentence. Instead of a phrase-structure hierarchy (like TGG or SG), it posits that one word (the head) governs or licenses another word (its dependent).</li> <li>Key Features: <ul> <li>Head-Dependent Relations: Every word in a sentence (except the main verb in some analyses) is a dependent of exactly one other word, or it’s the head of a clause.</li> <li>No Phrase Nodes: Typically doesn’t use non-terminal phrase nodes (like NP, VP) directly, although such concepts might be implicitly captured by the dependency relations.</li> <li>Direct Lexical Relations: Emphasizes direct grammatical relations between words.</li> <li>Cross-Linguistic Applicability: Often considered well-suited for languages with freer word order.</li> <li>Representation: Often uses dependency trees where arrows point from the head to its dependent.</li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="6-functional-grammar-fg">6. Functional Grammar (FG)</h2> <ul> <li>Core Idea: A broad umbrella term for theories that prioritize the function of language in communication, rather than focusing solely on its formal structure. Language is seen as a tool for interaction and meaning-making.</li> <li>Key Features (often seen in theories like Systemic Functional Linguistics by M.A.K. Halliday): <ul> <li>Meaning-Oriented: Emphasizes how grammatical choices contribute to meaning in context.</li> <li>Contextual: Analyzes language in relation to its social and cultural context.</li> <li>Metafunctions: Halliday proposes three primary metafunctions of language: Ideational (representing experience), Interpersonal (enacting social relations), and Textual (organizing discourse).</li> <li>Choices and Systems: Views grammar as a system of choices that speakers make to achieve communicative goals.</li> <li>Usage-Based: Often aligns with the idea that grammar emerges from language use.</li> <li>Distinction from Formal Grammars: Unlike TGG, which often treats meaning as secondary to syntax, FG integrates meaning and function into the very fabric of grammatical description.</li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="7-lexical-functional-grammar-lfg">7. Lexical Functional Grammar (LFG)</h2> <ul> <li>Core Idea: Developed by Joan Bresnan and Ronald Kaplan as an alternative to TGG. It emphasizes the importance of lexical information and parallel levels of representation for syntactic structure and functional structure.</li> <li>Key Features: <ul> <li>Parallel Structures: Maintains two primary levels of representation:</li> <li>C-structure (Constituent Structure): A phrase-structure tree representing the linear order and grouping of words.</li> <li>F-structure (Functional Structure): A set of attribute-value pairs representing grammatical functions (subject, object, oblique, etc.) and semantic roles.</li> <li>Lexicalism: Much of the grammatical information and constraints are stored in the lexicon (the mental dictionary), rather than being derived by complex transformational rules.</li> <li>Non-Derivational: Unlike TGG, LFG is not transformational; instead, it uses principles of unification to link c-structure and f-structure.</li> <li>Completeness and Coherence: Constraints ensure that f-structures are well-formed and contain all necessary information.</li> <li>Focus: Particularly strong in describing grammatical relations and their mapping to argument structure.</li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="8-construction-grammar-cg">8. Construction Grammar (CG)</h2> <ul> <li>Core Idea: A family of theories that argues that grammatical knowledge consists of a network of “constructions,” which are conventional pairings of form and meaning. These constructions range from individual words to complex sentence patterns.</li> <li>Key Features: <ul> <li>Construction: The central unit of analysis. A construction is a form-meaning pairing, where the form can be anything from a morpheme to a complex syntactic pattern, and the meaning can be lexical, idiomatic, or abstract grammatical meaning.</li> <li>Usage-Based: Emphasizes that grammatical knowledge is acquired and organized through experience with language use.</li> <li>Continuum of Specificity: Constructions exist at various levels of abstraction, from concrete idioms (e.g., “kick the bucket”) to highly abstract grammatical patterns (e.g., the ditransitive construction “Subj V Obj1 Obj2”).</li> <li>Inheritance Hierarchies: Constructions are related to each other through inheritance, forming a complex network.</li> <li>No Firm Form-Meaning Divide: Blurs the traditional distinction between lexicon and grammar.</li> <li>Varieties: Different schools exist, including Berkeley Construction Grammar (Fillmore, Kay, Lakoff), Radial Construction Grammar (Goldberg), and Embodied Construction Grammar.</li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="9-head-driven-phrase-structure-grammar-hpsg">9. Head-Driven Phrase Structure Grammar (HPSG)</h2> <ul> <li>Core Idea: Developed by Carl Pollard and Ivan Sag, HPSG is a constraint-based, lexicalist, and non-derivational theory that uses typed feature structures as its primary descriptive device.</li> <li>Key Features: <ul> <li>Feature Structures: All linguistic information (syntactic, semantic, phonological) is represented in complex data structures called “feature structures” (or “AVMs” - attribute-value matrices).</li> <li>Unification: Grammatical rules are essentially constraints that combine and unify these feature structures.</li> <li>Lexicalism: Like LFG, it places a heavy emphasis on the lexicon, where much of the grammatical information is stored.</li> <li>Sign-Based: Views linguistic expressions as “signs” that simultaneously contain phonetic, semantic, and syntactic information.</li> <li>Non-Derivational: Does not involve transformations between different levels of representation.</li> <li>Constraint-Based: Grammar is a system of interacting constraints rather than a series of sequential rules.</li> <li>Mathematical Sophistication: Known for its formal rigor and use of logic.</li> </ul> </li> </ul> <p><br/> <br/></p> <hr/> <p>The diagram you saw covers the “big nine,” but the field is a lot richer. Here are some of the notable theories missing from that picture, grouped a bit so it’s easier to digest:</p> <ul> <li><strong>Government &amp; Binding (GB)</strong> — 1980s generative framework with principles and parameters.</li> <li><strong>Minimalist Program</strong> — 1990s–present; Chomsky’s drive for economy and simplicity in grammar.</li> <li><strong>Tree-Adjoining Grammar (TAG)</strong> — formal system for syntax and semantics, strong in computational modeling.</li> <li><strong>Generalized Phrase Structure Grammar (GPSG)</strong> — precursor to HPSG, constraint-based.</li> <li><strong>Systemic Functional Grammar (SFG)</strong> — Halliday; grammar as a resource for meaning in context.</li> <li><strong>Prague School Functionalism</strong> — early 20th c.; emphasis on topic–comment and information structure.</li> <li><strong>Role and Reference Grammar (RRG)</strong> — links syntax, semantics, and discourse across languages.</li> <li><strong>Cognitive Grammar</strong> — Langacker; grammar as symbolic and conceptual structure.</li> <li><strong>Usage-based Models</strong> — Bybee, Tomasello; grammar shaped by frequency and analogy.</li> <li><strong>Embodied / Fluid Construction Grammar</strong> — constructionist models tied to cognition or computation.</li> <li><strong>Meaning–Text Theory (MTT)</strong> — Mel’čuk; semantically oriented, models paraphrase and surface variation.</li> <li><strong>Word Grammar</strong> — Hudson; dependency-based, network model of syntax.</li> <li><strong>Arc Pair Grammar</strong> — 1970s relational approach to grammatical functions.</li> <li><strong>Relational Grammar</strong> — focuses on grammatical relations like subject and object.</li> <li><strong>Combinatory Categorial Grammar (CCG)</strong> — highly compositional, strong in computational parsing.</li> <li><strong>DisCoCat (Distributional Compositional Categorical model)</strong> — recent hybrid of categorial grammar and vector-space semantics.</li> </ul>]]></content><author><name></name></author><category term="hi-linguistics"/><category term="theo-linguistics"/><category term="explanations"/><category term="review"/><category term="respaper"/><category term="grammar"/><category term="tg"/><category term="sg"/><category term="cg"/><category term="tgg"/><category term="dg"/><category term="fg"/><category term="lfg"/><category term="hpsg"/><summary type="html"><![CDATA[highlighting core principles and distinguishing features of each theory]]></summary></entry><entry><title type="html">why does my first linguistic paper fail terribly in advancing scientific inquiry?</title><link href="https://iamalinguist.github.io/blog/2025/review-of-syntactic-paper/" rel="alternate" type="text/html" title="why does my first linguistic paper fail terribly in advancing scientific inquiry?"/><published>2025-06-07T14:14:00+00:00</published><updated>2025-06-07T14:14:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/review-of-syntactic-paper</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/review-of-syntactic-paper/"><![CDATA[<p>I am going to refer to myself as the ‘author’ and talk about my <a href="https://iamalinguist.github.io/assets/pdf/article1.pdf">first paper</a>. This work takes a promising first step- but it remains more of a sketch than a robust theory. It lacks formal definitions, data evaluation, and engagement with canonical grammar theory is not at all present.</p> <p><br/> <br/></p> <h2 id="a-critical-review-of-the-paper">A. Critical review of the paper</h2> <hr/> <h5 id="1-unclear-theoretical-foundation">1. Unclear Theoretical Foundation</h5> <p>The author draws on two major theoretical foundations- <em>CFG (Chomsky)</em> and <em>Montague-style semantics</em>—under a unified framework, suggesting a possible ‘hybrid’ approach. However, upon examining <a href="https://iamalinguist.github.io/assets/pdf/article2.pdf">Part 2</a> of this article (and thus the complete paper), it becomes evident that it does not fully align with either theoretical model.</p> <ul> <li> <p>CFG is <em>Type 2 grammar</em> in the <em>Chomsky hierarchy</em>, and is more powerful than regular grammars (Type 3), but less powerful than context-sensitive grammars (Type 1) - author has not strictly followed the formalism and notations of Type 2 grammar.</p> </li> <li> <p>Montague grammar is tailored to <em>higher-order intensional logic</em> with <em>λ-calculus</em> and <em>quantificational structure</em> — yet author has restricted themselves to propositional logic, quantifier-less sentences, and simple entities.</p> </li> </ul> <p>Consequently, the use of <em>Chomsky grammar</em> or <em>Montague grammar</em> here appears unwarranted.</p> <p><br/> <br/></p> <h5 id="2-overly-simplistic-cfg-approach">2. Overly Simplistic CFG Approach</h5> <ul> <li> <p>It’s well-known that while CFGs can model many structures in natural languages, they struggle with free word order and agreement phenomena without extensions like TGG, LFG, GPSG, or MG (i.e. Minimalist Program).</p> </li> <li> <p>Author restricted grammar to rigid rule‑based word order thus can not deal with free word-order (scrambling).</p> </li> </ul> <p>If the author intended to use a context-free grammar (CFG), there is no formal definition of V, Σ, P and S. The author directly introduced the rules without specifying which symbols are terminals or non-terminals, nor did they indicate the starting point.</p> <p><br/> <br/></p> <h5 id="3-incomplete-formalization-of-gender-agreement-and-negation">3. Incomplete Formalization of Gender, Agreement, and Negation</h5> <ul> <li> <p>Gender misalignment detection is hard-coded rather than derived from structural or feature-based mechanisms, again weakening the design.</p> </li> <li> <p>Author introduced auxiliary‑gender agreement rules informally but don’t formalize them. By formalisation I mean defining feature structures or agreement constraints in the grammar.</p> </li> <li> <p>Negation is handled via ad hoc phrase transformation (X neg Y → neg X Y), but you never define a transformation grammar nor provide a normalization algorithm. This blurs the line between syntax and morphology and butchers modularity.</p> </li> </ul> <p><br/> <br/></p> <h5 id="4-evaluation-corpus-and-results-are-underwhelming">4. Evaluation: Corpus and Results Are Underwhelming</h5> <p>A proper evaluation section is missing. Author mentioned a “corpus … created” for experiments, but never provided corpus size, sources, annotation procedure, or detailed evaluation metrics (precision, recall, parsing accuracy). This lack of transparency prevents any assessment of performance or reproducibility.</p> <ul> <li>Examples 1–8 are illustrative, but no statistical data (e.g., coverage, failure rate, speed) is reported.</li> </ul> <p><br/> <br/></p> <h5 id="5-grammar-theoretic-gaps-and-missing-dependencies">5. Grammar Theoretic Gaps and Missing Dependencies</h5> <p>Key Hindi-specific linguistic phenomena aren’t addressed: postposition attachment, case marking beyond nominative/accusative, compound verbs, participles, adjectives/adverbs, adjective agreement, or complex embedding.</p> <ul> <li> <p>Author ignored discontinuous dependencies, long-distance movement, and scrambler constructs, which are central to modeling free word‑order languages.</p> </li> <li> <p>There’s no discussion of deeper categorization: feature structures for number, case, animacy, tense/mood/aspect, or the role of verbal agreement outside the auxiliary.</p> </li> </ul> <p><br/> <br/> <br/> <br/></p> <h2 id="b-grammatical-theory-framework-system">B. Grammatical Theory/ Framework/ System</h2> <hr/> <p>This paper does not clearly follow any of the major grammatical theories— at least not in a consistent or principled way. By grammatical theories, I mean theories like- Traditional Grammar (TG), Structural Grammar (SG), Transformational-Generative Grammar (TGG), Functional Grammar (FG), Dependency Grammar (DG), Lexical Functional Grammar (LFG), Head-Driven Phrase Structure Grammar (HPSG), Construction Grammar (CG) etc.</p> <table> <thead> <tr> <th>Grammatical Theory</th> <th>Does Paper Follow It?</th> <th>Why / Why Not</th> </tr> </thead> <tbody> <tr> <td>Traditional Grammar (TG)</td> <td>❌</td> <td>Not descriptive or terminology-based; author aims for formal rules.</td> </tr> <tr> <td>Structural Grammar (SG)</td> <td>❌</td> <td>Author has phrase rules, but SG focuses on distributions, not formal parsing.</td> </tr> <tr> <td>Transformational-Generative Grammar (TGG)</td> <td>❌</td> <td>No transformations (movement, deletion, etc.), no deep/surface structure.</td> </tr> <tr> <td>Functional Grammar (FG)</td> <td>❌</td> <td>No thematic roles, no discourse function labels, no functional motivation.</td> </tr> <tr> <td>Dependency Grammar (DG)</td> <td>❌</td> <td>No head-dependent relations, no dependency trees.</td> </tr> <tr> <td>Lexical Functional Grammar (LFG)</td> <td>❌</td> <td>No f-structures or c-structures, no mapping from lexical entries to syntax.</td> </tr> <tr> <td>Head-Driven Phrase Structure Grammar (HPSG)</td> <td>❌</td> <td>No feature structures, no unification, no typed logic.</td> </tr> <tr> <td>Construction Grammar (CG)</td> <td>❌</td> <td>No form-meaning pairings or construction-level generalizations.</td> </tr> </tbody> </table> <p><br/> <br/> <br/> This paper attempted a simplified CFG-based analysis, loosely reminiscent of early Context-Free Phrase Structure Grammars as used in early NLP (e.g., before the statistical revolution). If author wants to improve this work and make it publishable in a stronger venue, choosing one coherent grammatical theory and sticking with it will greatly help. Some suggestions:</p> <p><br/></p> <h6 id="option-1-dependency-grammar-ud-style"><strong>Option 1: Dependency Grammar (UD-style)</strong></h6> <p>If author is interested in parser implementation and wish to move toward NLP applications, DG is a better choice. It’s flexible and aligns with existing corpora (e.g., Hindi UD Treebank). Use head-modifier dependencies, model relations like <code class="language-plaintext highlighter-rouge">nsubj</code>, <code class="language-plaintext highlighter-rouge">obj</code>, <code class="language-plaintext highlighter-rouge">aux</code>, and handle scrambling via <code class="language-plaintext highlighter-rouge">edge labels</code>.</p> <p><br/></p> <h6 id="option-2-lexical-functional-grammar-lfg"><strong>Option 2: Lexical Functional Grammar (LFG)</strong></h6> <p>LFG is good for free word order languages like Hindi. Allows <a href="">c-structure and f-structure</a> separation. You can model:</p> <ul> <li>Surface word order using phrase structure rules (c-structure)</li> <li>Underlying relations (subject, object) using functional structures (f-structure)</li> <li>Gender, number, case as feature attributes</li> </ul> <p><br/></p> <h6 id="option-3-hpsg-head-driven-phrase-structure-grammar"><strong>Option 3: HPSG (Head-Driven Phrase Structure Grammar)</strong></h6> <p>HPSG is also great for morphologically rich languages. Rich in typed feature structures and lexical specifications. It is excellent for: Agreement modelling, Subject/object marking and Constraint-based parsing.</p> <p><br/> <br/> <br/> <br/></p> <h2 id="summary-and-improvement">Summary and Improvement</h2> <hr/> <p>Some major points of improvement for this paper are:</p> <ul> <li> <p>Clarify theoretical framework: state whether this is CFG-based Montague semantics or a hybrid, and defend that choice.</p> </li> <li> <p>Fully enumerate CFG rules (means listing all grammar rules in an ordered, numbered way, following formal notation.) in a consistent notation, including feature-attributes (gender, number, case, etc.).</p> </li> <li> <p>Expand grammar coverage to include optional orderings and scrambling.</p> </li> <li> <p>Formalize gender/agreement/negation using features or typed structures rather than ad hoc transformations.</p> </li> <li> <p>Provide corpus and evaluation metrics: size, domain, parsing accuracy, coverage, ambiguity rates.</p> </li> <li> <p>Publish code or grammar file for reproducibility and allow community verification.</p> </li> <li> <p>Compare against baselines: dependency parsers, formal grammar systems (GPSG, LFG, CG etc.).</p> </li> <li> <p>Address ambiguity management and discuss parse selection or ranking strategies.</p> </li> </ul>]]></content><author><name></name></author><category term="hi-linguistics"/><category term="theo-linguistics"/><category term="explanations"/><category term="review"/><category term="respaper"/><category term="grammar"/><category term="syntax"/><category term="cfg"/><summary type="html"><![CDATA[it remains more of a sketch than a robust theory]]></summary></entry><entry><title type="html">subjecthood in hindi</title><link href="https://iamalinguist.github.io/blog/2025/understanding-idea-of-subject/" rel="alternate" type="text/html" title="subjecthood in hindi"/><published>2025-06-07T00:32:13+00:00</published><updated>2025-06-07T00:32:13+00:00</updated><id>https://iamalinguist.github.io/blog/2025/understanding-idea-of-subject</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/understanding-idea-of-subject/"><![CDATA[<p>Understanding what constitutes a subject in linguistics is a deceptively complex issue, especially when examined across the world’s languages. Unlike many grammatical terms (like verb or noun), “subject” does not have a universally agreed-upon definition in theoretical or descriptive linguistics. This confusion stems from the fact that languages vary widely in how they encode grammatical relations and in what they treat as “subject-like” elements.</p> <p>In syntactic theory, core subject properties are diagnostic behaviors typically used to identify the subject in a sentence. The main ones are:</p> <ul id="subjecthood-tests" class="tab" data-tab="62b6aae1-b692-4adf-92ab-8c5a2edeeffc" data-name="subjecthood-tests"> <li class="active" id="subjecthood-tests-control-of-pro"> <a href="#">Control of PRO </a> </li> <li id="subjecthood-tests-reflexive-binding"> <a href="#">Reflexive Binding </a> </li> <li id="subjecthood-tests-raising"> <a href="#">Raising </a> </li> <li id="subjecthood-tests-agreement"> <a href="#">Agreement </a> </li> <li id="subjecthood-tests-subject-oriented-adverbials"> <a href="#">Subject-Oriented Adverbials </a> </li> <li id="subjecthood-tests-discourse-prominence"> <a href="#">Discourse Prominence </a> </li> </ul> <ul class="tab-content" id="62b6aae1-b692-4adf-92ab-8c5a2edeeffc" data-name="subjecthood-tests"> <li class="active"> <div style="background-color: #f0f0f0; padding: 10px; border: 2px solid #007bff;"> Subject controls the understood subject in non-finite clauses. <br/> → Ram [PRO khelne gaya]. ("Ram went [to play].") </div> </li> <li> <div style="background-color: #f0f0f0; padding: 10px; border: 2px solid #007bff;"> Subject can bind reflexive pronouns. <br/> → Ram-ne apne aap ko dekha. ("Ram saw himself.") </div> </li> <li> <div style="background-color: #f0f0f0; padding: 10px; border: 2px solid #007bff;"> Subject can raise from embedded clauses. <br/> → Ram lagta hai [_ bimaar hai]._ ("Ram seems to be sick.") </div> </li> <li> <div style="background-color: #f0f0f0; padding: 10px; border: 2px solid #007bff;"> Subject usually triggers verb agreement. <br/> → Ladki ja rahi hai. ("The girl is going.") → agreement in number/gender. </div> </li> <li> <div style="background-color: #f0f0f0; padding: 10px; border: 2px solid #007bff;"> Some adverbs refer only to the subject. <br/> → Ram jaan-bujhkar gira. ("Ram fell on purpose.") → “on purpose” must modify the subject. </div> </li> <li> <div style="background-color: #f0f0f0; padding: 10px; border: 2px solid #007bff;"> Subject tends to be the topic or focus. <br/> → Often sentence-initial in neutral word order. </div> </li> </ul> <p><br/></p> <p>These tests are used to identify subjecthood even when case marking is non-nominative, especially in Hindi.</p> <p><br/></p> <h4 id="1-syntactic-minimalism-and-argument-structure">1. Syntactic Minimalism and Argument Structure</h4> <p>Mohanan (1994) presented a pivotal generative account where subjecthood in Hindi is not determined by surface case (like nominative), but by <strong>argument hierarchy</strong> and <strong>syntactic configuration</strong>. She argued that ergative-marked arguments (e.g., <em>“Ram-ne”</em> in <em>“Ram-ne roti khayi”</em>) are <strong>true syntactic subjects</strong> despite their non-nominative case.</p> <blockquote> <table> <tbody> <tr> <td>Reference: Mohanan, T. (1994). <em>Argument Structure in Hindi</em>. Stanford: CSLI Publications.</td> <td>📘<a href="https://books.google.com/books?id=Pnyo3SJTMngC">Link</a></td> </tr> </tbody> </table> </blockquote> <hr/> <p><br/> <br/></p> <h4 id="2-non-nominative-subjects-and-voice-syntax">2. Non-nominative Subjects and Voice Syntax</h4> <p>Montaut (2008, 2013) developed a typology of <em>non-canonical subjecthood</em> in Hindi, showing that <strong>oblique and dative-marked arguments</strong> often display subject properties—control, binding, and topicality—even when they are not agents.</p> <blockquote> <table> <tbody> <tr> <td>Reference: Montaut, A. (2008). <em>Oblique Main Arguments in Hindi</em>. In <em>Non-Nominative Subjects Vol. 2</em>.</td> <td>📖<a href="https://www.degruyter.com/document/doi/10.1075/tsl.61.04mon/html">PDF</a></td> </tr> </tbody> </table> </blockquote> <hr/> <p><br/> <br/></p> <h4 id="3-subject-as-a-gradient-not-a-category">3. Subject as a Gradient, Not a Category</h4> <p>Ethan Poole (2016) proposed that subjecthood in Hindi is <strong>property-based and gradient</strong>. He showed that ergative or dative arguments may lack some properties (e.g., agreement) but retain others (e.g., control over PRO).</p> <blockquote> <table> <tbody> <tr> <td>Reference: Poole, E. (2016). <em>Deconstructing Subjecthood</em>.</td> <td>📖 <a href="https://ethanpoole.com/papers/poole-2016-subjecthood.pdf">PDF</a></td> </tr> </tbody> </table> </blockquote> <hr/> <p><br/> <br/></p> <h4 id="4-morphological-ergative--aspectual-condition">4. Morphological Ergative &amp; Aspectual Condition</h4> <p>Finley (2010) argued that the presence of <em>-ne</em> is conditioned by <strong>aspect</strong>, not subjecthood. In Hindi, the perfective aspect determines ergative alignment, breaking down a direct link between morphology and syntactic function.</p> <blockquote> <table> <tbody> <tr> <td>Reference: Finley, J. (2010). <em>Aspectually-conditioned morphological ergativity in Hindi</em>.</td> <td>📖 <a href="https://archipel.uqam.ca/3870/1/D1976.pdf">PDF</a></td> </tr> </tbody> </table> </blockquote> <hr/> <p><br/> <br/></p> <h4 id="5-comparative-syntax-diachrony-and-function">5. Comparative Syntax: Diachrony and Function</h4> <p>Butt (2012) traced the evolution of the <em>ne-marker</em> from a <strong>spatial marker to a subject marker</strong>, suggesting that subjecthood in Hindi is <strong>historically emergent</strong> and functionally aligned.</p> <blockquote> <table> <tbody> <tr> <td>Reference: Butt, M. (2012). <em>From Spatial to Subject Marker</em>.</td> <td>📖 <a href="https://ling.sprachwiss.uni-konstanz.de/pages/home/butt/main/papers/iceland12-hnd.pdf">PDF</a></td> </tr> </tbody> </table> </blockquote> <hr/> <p><br/> <br/></p> <h4 id="6-subjecthood-tests-in-hindi">6. Subjecthood Tests in Hindi</h4> <p>Fatma (2023) applied classical diagnostics such as <strong>binding, control, and adverbial orientation</strong> to show that Hindi licenses <strong>multiple subject types</strong>, including ergative, dative, and oblique arguments.</p> <blockquote> <table> <tbody> <tr> <td>Reference: Fatma, Z. (2023). <em>Non-nominative Subjects in Kannauji and Hindi-Urdu</em>.</td> <td>📖 <a href="https://linguistics.uok.edu.in/Files/f6ec3740-422d-4ac1-9f52-ddfe2cffcb28/Journal/e0ccce10-07e2-4088-a575-28b943e4ece8.pdf">PDF</a></td> </tr> </tbody> </table> </blockquote> <hr/> <p><br/> <br/></p> <h4 id="7-karaka-theory-and-semantic-alignment">7. Karaka Theory and Semantic Alignment</h4> <p>Debnath (2021) used <strong>Paninian grammar and pregroup logic</strong> to show that karaka relations in Hindi reflect <strong>semantic roles</strong>, which sometimes override morphological case in determining subject-like behavior.</p> <blockquote> <table> <tbody> <tr> <td>Reference: Debnath, A. (2021). <em>A Pregroup Representation and Analysis of Hindi Syntax</em>.</td> <td>📖 <a href="https://cdn.iiit.ac.in/cdn/web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf">PDF</a></td> </tr> </tbody> </table> </blockquote> <hr/> <p><br/> <br/></p> <h4 id="8-binding-theory-and-voice">8. Binding Theory and Voice⁰</h4> <p>Bhatia &amp; Poole (2016) showed that <strong>Voice⁰</strong>, the head that introduces external arguments, determines <strong>subject-oriented adverbial behavior</strong>. Thus, subjecthood in Hindi is derivationally encoded, not simply morphological.</p> <blockquote> <table> <tbody> <tr> <td>Reference: Bhatia, S., &amp; Poole, E. (2016). <em>Deriving Subject and Antisubject Orientation</em>.</td> <td>📖 <a href="https://ojs.ub.uni-konstanz.de/jsal/index.php/fasal/article/view/104/62">PDF</a></td> </tr> </tbody> </table> </blockquote> <hr/> <p><br/> <br/></p> <p>✅ <strong>Final Synthesis:</strong> A Research Proposition on Subjecthood in Hindi- Drawing from these foundational works, I propose the following <strong>testable definition</strong>:</p> <blockquote> <p><strong>In Hindi, a subject is any argument that satisfies a critical mass of syntactic and semantic diagnostics — including control over PRO, binding of reflexives, raising behavior, subject-oriented adverbial licensing, and discourse prominence — regardless of morphological case or surface position.</strong></p> </blockquote> <p>This means <strong>subjecthood is not tied to nominative or ergative morphology</strong>, but instead arises from <strong>syntactic behavior and functional prominence</strong>.</p> <hr/> <p><br/> <br/></p> <h3 id="-validating-examples">🔬 Validating Examples</h3> <table> <thead> <tr> <th>Construction Type</th> <th>Example</th> <th>Subject Properties Satisfied</th> </tr> </thead> <tbody> <tr> <td>Nominative</td> <td><em>Ravi so raha hai</em></td> <td>✔ Agreement, ✔ Binding</td> </tr> <tr> <td>Ergative</td> <td><em>Ravi-ne roti khayi</em></td> <td>✔ Control, ✔ Subject role</td> </tr> <tr> <td>Dative (Experiencer)</td> <td><em>Mujhe gussa aaya</em></td> <td>✔ Reflexive Binding</td> </tr> <tr> <td>Oblique (Control Test)</td> <td><em>Mujhse apne aap par vishwas…</em></td> <td>✔ Reflexive Binding</td> </tr> <tr> <td>Raising Construction</td> <td><em>Ram lagta hai [__ bimaar hai]</em></td> <td>✔ Raising</td> </tr> </tbody> </table> <hr/> <p><strong>NOTE:</strong> This research framework welcomes expansion through dialectal data, corpus validation, and experimental psycholinguistic testing, making it ideal for advanced comparative and South Asian linguistic work.</p>]]></content><author><name></name></author><category term="hi-linguistics"/><category term="theo-linguistics"/><category term="explanations"/><category term="research"/><category term="subjecthood"/><category term="syntax"/><category term="minimalism"/><category term="argument"/><category term="karaka"/><summary type="html"><![CDATA[A Research Proposition on Subjecthood in Hindi]]></summary></entry><entry><title type="html">a note for ‘linguistics + nlp’ domain</title><link href="https://iamalinguist.github.io/blog/2025/a-note-ling-nlp/" rel="alternate" type="text/html" title="a note for ‘linguistics + nlp’ domain"/><published>2025-05-31T09:00:00+00:00</published><updated>2025-05-31T09:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/a-note-ling-nlp</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/a-note-ling-nlp/"><![CDATA[<p>While Linguistics seeks to provide the formal, theoretical, and descriptive tools to understand the human language, NLP seeks to computationally model it (for tasks like translation, parsing, sentiment analysis etc.).</p> <p>Combining Linguistics with NLP leads to <em>robust</em>, <em>explainable</em>, and <em>cross-linguistically aware</em> models. While modern deep learning approaches in NLP sometimes bypass linguistic theory, the long-term sustainability of language technologies—especially for low-resource, morphologically rich, or culturally embedded languages—depends on linguistic insight.</p> <p><br/></p> <blockquote> <p>This interdisciplinary journey is not just about building tools that “work”, but about building systems that “understand”.</p> </blockquote> <p><br/> This section outlines key domains/research areas where such integration is both necessary and promising.</p> <hr/> <p><br/> <br/></p> <h2 id="1-formal-grammars-in-nlp">1. Formal Grammars in NLP</h2> <p>At the research frontier, the syntactic structure of natural languages is no longer modeled with simple rules. Instead, linguistically informed <strong>formal grammars</strong> such as Tree-Adjoining Grammar (TAG), Combinatory Categorial Grammar (CCG), Head-driven Phrase Structure Grammar (HPSG), and Minimalist Grammars (MG) are employed. These frameworks capture long-distance dependencies, coordination, and movement more naturally than context-free grammars (CFGs).</p> <ul> <li><strong>TAG</strong>: Suitable for modeling recursion and crossing dependencies.</li> <li><strong>CCG</strong>: Offers transparent syntax-semantics mapping using combinatory logic.</li> <li><strong>MG</strong>: Grounded in Chomsky’s Minimalist Program, it provides a generative account of human language using operations like <em>Merge</em> and <em>Move</em>.</li> </ul> <p>Such grammars are essential for <em>syntactic parsing</em>, especially in linguistically complex or low-resource languages.</p> <hr/> <p><br/> <br/></p> <h2 id="2-compositional-semantics-and-lambda-calculus">2. Compositional Semantics and Lambda Calculus</h2> <p>Advanced semantic modeling involves <strong>compositionality</strong>—the principle that the meaning of a sentence is determined by its parts and their arrangement. This is formalized using <em>typed lambda calculus</em>, where:</p> <ul> <li>Noun phrases, verbs, and modifiers are treated as functions or arguments.</li> <li>Complex meanings are built incrementally by function application.</li> </ul> <p><em>Montague Semantics</em>, <em>Intensional Logic</em>, and <em>Dynamic Semantics</em> offer logical systems to model ambiguity, quantification, modality, and discourse reference. (These topics are widely new to me).</p> <hr/> <p><br/> <br/></p> <h2 id="3-abstract-meaning-representation-amr-and-graph-based-semantics">3. Abstract Meaning Representation (AMR) and Graph-Based Semantics</h2> <p>Abstract Meaning Representation (AMR) provides a graph-based formalism for sentence meaning. Unlike trees, AMR allows multiple incoming edges, reentrancy, and coreference, making it suitable for representing:</p> <ul> <li> <p>Entity relationships</p> </li> <li> <p>Events and arguments</p> </li> <li> <p>Quantification and scope</p> </li> <li> <p>Modality and negation</p> </li> </ul> <p><strong>AMR parsing</strong> is a key task in computational semantics and is central to information extraction, question answering, and knowledge integration.</p> <hr/> <p><br/> <br/></p> <h2 id="4-discourse-and-dynamic-semantics">4. Discourse and Dynamic Semantics</h2> <p>Modeling extended texts and dialogues requires going beyond sentence-level semantics:</p> <ul> <li> <p>Discourse Representation Theory (DRT) introduces intermediate structures for handling anaphora and temporal relations.</p> </li> <li> <p>Dynamic Semantics (like Dynamic Predicate Logic) allows meanings to update the “context state” as discourse progresses.</p> </li> </ul> <p>Such frameworks are crucial for coreference resolution, dialogue systems, and narrative understanding.</p> <hr/> <p><br/> <br/></p> <h2 id="5-multilinguality-and-linguistic-typology-in-nlp">5. Multilinguality and Linguistic Typology in NLP</h2> <p>Research-level NLP increasingly addresses the typological diversity of the world’s languages:</p> <ul> <li> <p>Projects like Universal Dependencies (UD) aim to standardize syntactic annotation across languages.</p> </li> <li> <p>Typological Databases (e.g., WALS, PHOIBLE) inform models about word order, morphology, and phonological inventories.</p> </li> </ul> <p>In multilingual NLP:</p> <ul> <li> <p>Transfer learning and multilingual LLMs (e.g., mBERT, XLM-R) help generalize across languages.</p> </li> <li> <p>Linguistically grounded models improve performance on low-resource languages by leveraging structural knowledge.</p> </li> </ul> <hr/> <p><br/> <br/></p> <h2 id="6-inference-world-knowledge-and-commonsense-reasoning">6. Inference, World Knowledge, and Commonsense Reasoning</h2> <p>Semantic processing increasingly intersects with AI reasoning:</p> <ul> <li> <p>Natural Logic and Entailment Systems evaluate valid inferences from language.</p> </li> <li> <p>Commonsense reasoning tasks (e.g., Winograd Schema, Story Cloze Test) demand real-world background knowledge.</p> </li> <li> <p>Ontologies like FrameNet, VerbNet, and ConceptNet provide structured knowledge for event semantics and role labeling.</p> </li> </ul> <p>Inference also plays a role in explainable NLP, where output must be justified with traceable logical steps.</p> <hr/> <p><br/> <br/></p> <h2 id="7-language-and-logic-type-theory-and-higher-order-semantics">7. Language and Logic: Type Theory and Higher-Order Semantics</h2> <p>Advanced systems often adopt Type Theory and Categorial Grammar to provide highly compositional, structured accounts of meaning. Examples include:</p> <ul> <li> <p>Montague Grammar: Uses types e (entity) and t (truth value) to build meaning recursively.</p> </li> <li> <p>λμ Calculus (Lambda Mu Calculus), Linear Logic, and Dependent Type Theory: Extend the expressive power of semantic models.</p> </li> </ul> <hr/> <p><br/> <br/></p> <p><strong>Conclusion</strong>: Advanced topics in Linguistics + NLP reflect the need for interpretable, generalizable, and cognitively aligned models. As language models grow in size and capability, their alignment with formal linguistic theories becomes both a challenge and an opportunity.</p> <p><br/> <br/></p> <h2 id="recommended-reading--resources">Recommended Reading &amp; Resources</h2> <ul> <li>Chomsky - <em>Syntactic Structures</em></li> <li>Jacob Eisenstein - <em>Natural Language Processing</em></li> <li><a href="https://aclanthology.org">ACL Anthology</a> - The Association for Computational Linguistics</li> </ul>]]></content><author><name></name></author><category term="theo-linguistics"/><category term="explanations"/><category term="grammar"/><category term="nlp"/><category term="parsing"/><category term="parser"/><category term="logic"/><category term="semantics"/><summary type="html"><![CDATA[A Comprehensive Guide to Key Research Areas]]></summary></entry></feed>