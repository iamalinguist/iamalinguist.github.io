<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://iamalinguist.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iamalinguist.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-13T06:20:49+00:00</updated><id>https://iamalinguist.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">an overview of the grammatical theories</title><link href="https://iamalinguist.github.io/blog/2025/grammatical-theories/" rel="alternate" type="text/html" title="an overview of the grammatical theories"/><published>2025-06-07T21:01:00+00:00</published><updated>2025-06-07T21:01:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/grammatical-theories</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/grammatical-theories/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/chronology-480.webp 480w,/assets/img/chronology-800.webp 800w,/assets/img/chronology-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/chronology.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Timeline of Major Grammar Theories: Traditional Grammar (TG), Structural Grammar (SG), Transformational-Generative Grammar (TGG), Construction Grammar (CG), Categorial Grammar (CG), Functional Grammar (FG), Dependency Grammar (DG), Lexical Functional Grammar (LFG) and Head-Driven Phrase Structure Grammar (HPSG). </div> <p><br/> <br/> <br/></p> <h2 id="1-traditional-grammar-tg">1. Traditional Grammar (TG)</h2> <ul> <li>Period of Dominance: Predominantly before the 20th century, though its influence persists.</li> <li>Core Idea: Rooted in ancient Greek and Latin grammar, its principles have been influential for centuries, dating back to classical antiquity (e.g., Pāṇini in 6th-5th century BC India, Dionysius Thrax in 2nd century BC Greece). It focuses on prescribing “correct” usage based on established rules, often viewing Latin as the ideal model for all languages.</li> <li>Key Features: <ul> <li>Prescriptive: Dictates how language should be used, rather than describing how it is used.</li> <li>Categorization: Emphasizes parts of speech (noun, verb, adjective, etc.) and their morphological forms (e.g., conjugations, declensions).</li> <li>Syntactic Analysis: Primarily sentence-level analysis, often using parsing methods to identify subject, predicate, object, etc.</li> <li>Focus on Meaning: Relies heavily on semantic notions (e.g., “a noun is the name of a person, place, or thing”).</li> <li> <h2 id="limitations-often-struggles-with-the-diversity-and-fluidity-of-natural-language-sometimes-imposing-rules-that-dont-reflect-actual-usage">Limitations: Often struggles with the diversity and fluidity of natural language, sometimes imposing rules that don’t reflect actual usage.</h2> </li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="2-structural-grammar-sg">2. Structural Grammar (SG)</h2> <ul> <li>Period of Dominance: Mid-20th century (roughly 1930s-1960s).</li> <li>Core Idea: Developed as a reaction against the prescriptivism and semantic basis of Traditional Grammar. It advocates for the scientific study of language by focusing on observable linguistic data (utterances) and their distribution. Influenced by Bloomfieldian linguistics.</li> <li>Key Features: <ul> <li>Descriptive: Aims to describe how language is used, without imposing external norms.</li> <li>Empirical: Relies on analyzing actual language data.</li> <li>Distributional Analysis: Identifies linguistic units (phonemes, morphemes, words) based on where and how they occur in relation to other units.</li> <li>Immediate Constituent Analysis (ICA): A method for breaking down sentences into their immediate structural components, often represented by tree diagrams.</li> <li>Form over Meaning: Prioritizes the formal arrangement of linguistic elements over their meaning in analysis.</li> <li> <h2 id="limitations-often-criticized-for-not-adequately-accounting-for-the-creativity-of-language-or-the-speakers-underlying-knowledge">Limitations: Often criticized for not adequately accounting for the creativity of language or the speaker’s underlying knowledge.</h2> </li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="3-categorial-grammar-cg">3. Categorial Grammar (CG)</h2> <ul> <li>Core Idea: Views grammar as a system of types, where words are assigned categories that specify their combinatory properties. It is a highly formal and mathematically oriented approach.</li> <li>Key Features: <ul> <li>Functional Application: The primary mechanism for combining linguistic units. Categories are often represented as “functions” that take arguments and produce new categories.</li> <li>Lexicalized: All syntactic information is encoded in the lexical entries of words.</li> <li>Directionality: Categories specify the direction in which they combine with other categories (e.g., a verb might take a noun phrase to its right).</li> <li>No Phrase Structure Rules: Syntactic structure emerges directly from the combination of lexical categories.</li> <li>Semantic Transparency: Often aims for a direct mapping between syntactic structure and semantic interpretation.</li> <li> <h2 id="varieties-different-systems-exist-such-as-the-lambek-calculus-which-has-strong-connections-to-mathematical-logic">Varieties: Different systems exist, such as the Lambek Calculus, which has strong connections to mathematical logic.</h2> </li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="4-transformational-generative-grammar-tgg">4. Transformational-Generative Grammar (TGG)</h2> <ul> <li>Period of Dominance: From the late 1950s onwards, heavily associated with Noam Chomsky.</li> <li>Core Idea: A significant paradigm shift that proposed that linguistic knowledge is innate and that speakers possess a “generative” grammar that allows them to produce and understand an infinite number of sentences. It distinguishes between deep structure (the underlying meaning) and surface structure (the actual spoken or written form).</li> <li>Key Features: <ul> <li>Generative: Aims to provide a set of explicit rules that can generate all and only the grammatical sentences of a language.</li> <li>Competence vs. Performance: Distinguishes between a speaker’s underlying linguistic knowledge (competence) and their actual use of language (performance).</li> <li>Deep Structure &amp; Surface Structure: Sentences have an abstract deep structure (representing core meaning and grammatical relations) and a surface structure (how they are pronounced). Transformations map deep structures to surface structures.</li> <li>Universal Grammar (UG): Proposes that humans are born with an innate linguistic faculty that provides universal principles common to all languages.</li> <li>Recursion: Explains how language can generate infinitely long sentences through repetitive application of rules.</li> <li> <h2 id="evolution-has-undergone-several-revisions-eg-standard-theory-extended-standard-theory-government-and-binding-theory-minimalist-program">Evolution: Has undergone several revisions (e.g., Standard Theory, Extended Standard Theory, Government and Binding Theory, Minimalist Program).</h2> </li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="5-dependency-grammar-dg">5. Dependency Grammar (DG)</h2> <ul> <li>Core Idea: Focuses on dependency relations between words in a sentence. Instead of a phrase-structure hierarchy (like TGG or SG), it posits that one word (the head) governs or licenses another word (its dependent).</li> <li>Key Features: <ul> <li>Head-Dependent Relations: Every word in a sentence (except the main verb in some analyses) is a dependent of exactly one other word, or it’s the head of a clause.</li> <li>No Phrase Nodes: Typically doesn’t use non-terminal phrase nodes (like NP, VP) directly, although such concepts might be implicitly captured by the dependency relations.</li> <li>Direct Lexical Relations: Emphasizes direct grammatical relations between words.</li> <li>Cross-Linguistic Applicability: Often considered well-suited for languages with freer word order.</li> <li> <h2 id="representation-often-uses-dependency-trees-where-arrows-point-from-the-head-to-its-dependent">Representation: Often uses dependency trees where arrows point from the head to its dependent.</h2> </li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="6-functional-grammar-fg">6. Functional Grammar (FG)</h2> <ul> <li>Core Idea: A broad umbrella term for theories that prioritize the function of language in communication, rather than focusing solely on its formal structure. Language is seen as a tool for interaction and meaning-making.</li> <li>Key Features (often seen in theories like Systemic Functional Linguistics by M.A.K. Halliday): <ul> <li>Meaning-Oriented: Emphasizes how grammatical choices contribute to meaning in context.</li> <li>Contextual: Analyzes language in relation to its social and cultural context.</li> <li>Metafunctions: Halliday proposes three primary metafunctions of language: Ideational (representing experience), Interpersonal (enacting social relations), and Textual (organizing discourse).</li> <li>Choices and Systems: Views grammar as a system of choices that speakers make to achieve communicative goals.</li> <li>Usage-Based: Often aligns with the idea that grammar emerges from language use.</li> <li> <h2 id="distinction-from-formal-grammars-unlike-tgg-which-often-treats-meaning-as-secondary-to-syntax-fg-integrates-meaning-and-function-into-the-very-fabric-of-grammatical-description">Distinction from Formal Grammars: Unlike TGG, which often treats meaning as secondary to syntax, FG integrates meaning and function into the very fabric of grammatical description.</h2> </li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="7-lexical-functional-grammar-lfg">7. Lexical Functional Grammar (LFG)</h2> <ul> <li>Core Idea: Developed by Joan Bresnan and Ronald Kaplan as an alternative to TGG. It emphasizes the importance of lexical information and parallel levels of representation for syntactic structure and functional structure.</li> <li>Key Features: <ul> <li>Parallel Structures: Maintains two primary levels of representation:</li> <li>C-structure (Constituent Structure): A phrase-structure tree representing the linear order and grouping of words.</li> <li>F-structure (Functional Structure): A set of attribute-value pairs representing grammatical functions (subject, object, oblique, etc.) and semantic roles.</li> <li>Lexicalism: Much of the grammatical information and constraints are stored in the lexicon (the mental dictionary), rather than being derived by complex transformational rules.</li> <li>Non-Derivational: Unlike TGG, LFG is not transformational; instead, it uses principles of unification to link c-structure and f-structure.</li> <li>Completeness and Coherence: Constraints ensure that f-structures are well-formed and contain all necessary information.</li> <li> <h2 id="focus-particularly-strong-in-describing-grammatical-relations-and-their-mapping-to-argument-structure">Focus: Particularly strong in describing grammatical relations and their mapping to argument structure.</h2> </li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="8-construction-grammar-cg">8. Construction Grammar (CG)</h2> <ul> <li>Core Idea: A family of theories that argues that grammatical knowledge consists of a network of “constructions,” which are conventional pairings of form and meaning. These constructions range from individual words to complex sentence patterns.</li> <li>Key Features: <ul> <li>Construction: The central unit of analysis. A construction is a form-meaning pairing, where the form can be anything from a morpheme to a complex syntactic pattern, and the meaning can be lexical, idiomatic, or abstract grammatical meaning.</li> <li>Usage-Based: Emphasizes that grammatical knowledge is acquired and organized through experience with language use.</li> <li>Continuum of Specificity: Constructions exist at various levels of abstraction, from concrete idioms (e.g., “kick the bucket”) to highly abstract grammatical patterns (e.g., the ditransitive construction “Subj V Obj1 Obj2”).</li> <li>Inheritance Hierarchies: Constructions are related to each other through inheritance, forming a complex network.</li> <li>No Firm Form-Meaning Divide: Blurs the traditional distinction between lexicon and grammar.</li> <li> <h2 id="varieties-different-schools-exist-including-berkeley-construction-grammar-fillmore-kay-lakoff-radial-construction-grammar-goldberg-and-embodied-construction-grammar">Varieties: Different schools exist, including Berkeley Construction Grammar (Fillmore, Kay, Lakoff), Radial Construction Grammar (Goldberg), and Embodied Construction Grammar.</h2> </li> </ul> </li> </ul> <p><br/> <br/></p> <h2 id="9-head-driven-phrase-structure-grammar-hpsg">9. Head-Driven Phrase Structure Grammar (HPSG)</h2> <ul> <li>Core Idea: Developed by Carl Pollard and Ivan Sag, HPSG is a constraint-based, lexicalist, and non-derivational theory that uses typed feature structures as its primary descriptive device.</li> <li>Key Features: <ul> <li>Feature Structures: All linguistic information (syntactic, semantic, phonological) is represented in complex data structures called “feature structures” (or “AVMs” - attribute-value matrices).</li> <li>Unification: Grammatical rules are essentially constraints that combine and unify these feature structures.</li> <li>Lexicalism: Like LFG, it places a heavy emphasis on the lexicon, where much of the grammatical information is stored.</li> <li>Sign-Based: Views linguistic expressions as “signs” that simultaneously contain phonetic, semantic, and syntactic information.</li> <li>Non-Derivational: Does not involve transformations between different levels of representation.</li> <li>Constraint-Based: Grammar is a system of interacting constraints rather than a series of sequential rules.</li> <li> <h2 id="mathematical-sophistication-known-for-its-formal-rigor-and-use-of-logic">Mathematical Sophistication: Known for its formal rigor and use of logic.</h2> </li> </ul> </li> </ul> <p><br/> <br/></p>]]></content><author><name></name></author><category term="theo-linguistics"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[highlighting core principles and distinguishing features of each theory]]></summary></entry><entry><title type="html">Why does my first linguistic paper fail terribly in advancing scientific inquiry?</title><link href="https://iamalinguist.github.io/blog/2025/review-of-syntactic-paper/" rel="alternate" type="text/html" title="Why does my first linguistic paper fail terribly in advancing scientific inquiry?"/><published>2025-06-07T14:14:00+00:00</published><updated>2025-06-07T14:14:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/review-of-syntactic-paper</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/review-of-syntactic-paper/"><![CDATA[<p>I am going to refer to myself as the ‘author’ and talk about my <a href="https://iamalinguist.github.io/assets/pdf/article1.pdf">first paper</a>. This work takes a promising first step- but it remains more of a sketch than a robust theory. It lacks formal definitions, data evaluation, and engagement with canonical grammar theory is not at all present.</p> <p><br/> <br/></p> <h2 id="a-critical-review-of-the-paper">A. Critical review of the paper</h2> <hr/> <h5 id="1-unclear-theoretical-foundation">1. Unclear Theoretical Foundation</h5> <p>The author draws on two major theoretical foundations- <em>CFG (Chomsky)</em> and <em>Montague-style semantics</em>—under a unified framework, suggesting a possible ‘hybrid’ approach. However, upon examining <a href="https://iamalinguist.github.io/assets/pdf/article2.pdf">Part 2</a> of this article (and thus the complete paper), it becomes evident that it does not fully align with either theoretical model.</p> <ul> <li> <p>CFG is <em>Type 2 grammar</em> in the <em>Chomsky hierarchy</em>, and is more powerful than regular grammars (Type 3), but less powerful than context-sensitive grammars (Type 1) - author has not strictly followed the formalism and notations of Type 2 grammar.</p> </li> <li> <p>Montague grammar is tailored to <em>higher-order intensional logic</em> with <em>λ-calculus</em> and <em>quantificational structure</em> — yet author has restricted themselves to propositional logic, quantifier-less sentences, and simple entities.</p> </li> </ul> <p>Consequently, the use of <em>Chomsky grammar</em> or <em>Montague grammar</em> here appears unwarranted.</p> <p><br/> <br/></p> <h5 id="2-overly-simplistic-cfg-approach">2. Overly Simplistic CFG Approach</h5> <ul> <li> <p>It’s well-known that while CFGs can model many structures in natural languages, they struggle with free word order and agreement phenomena without extensions like TGG, LFG, GPSG, or MG (i.e. Minimalist Program).</p> </li> <li> <p>Author restricted grammar to rigid rule‑based word order thus can not deal with free word-order (scrambling).</p> </li> </ul> <p>If the author intended to use a context-free grammar (CFG), there is no formal definition of V, Σ, P and S. The author directly introduced the rules without specifying which symbols are terminals or non-terminals, nor did they indicate the starting point.</p> <p><br/> <br/></p> <h5 id="3-incomplete-formalization-of-gender-agreement-and-negation">3. Incomplete Formalization of Gender, Agreement, and Negation</h5> <ul> <li> <p>Gender misalignment detection is hard-coded rather than derived from structural or feature-based mechanisms, again weakening the design.</p> </li> <li> <p>Author introduced auxiliary‑gender agreement rules informally but don’t formalize them. By formalisation I mean defining feature structures or agreement constraints in the grammar.</p> </li> <li> <p>Negation is handled via ad hoc phrase transformation (X neg Y → neg X Y), but you never define a transformation grammar nor provide a normalization algorithm. This blurs the line between syntax and morphology and butchers modularity.</p> </li> </ul> <p><br/> <br/></p> <h5 id="4-evaluation-corpus-and-results-are-underwhelming">4. Evaluation: Corpus and Results Are Underwhelming</h5> <p>A proper evaluation section is missing. Author mentioned a “corpus … created” for experiments, but never provided corpus size, sources, annotation procedure, or detailed evaluation metrics (precision, recall, parsing accuracy). This lack of transparency prevents any assessment of performance or reproducibility.</p> <ul> <li>Examples 1–8 are illustrative, but no statistical data (e.g., coverage, failure rate, speed) is reported.</li> </ul> <p><br/> <br/></p> <h5 id="5-grammar-theoretic-gaps-and-missing-dependencies">5. Grammar Theoretic Gaps and Missing Dependencies</h5> <p>Key Hindi-specific linguistic phenomena aren’t addressed: postposition attachment, case marking beyond nominative/accusative, compound verbs, participles, adjectives/adverbs, adjective agreement, or complex embedding.</p> <ul> <li> <p>Author ignored discontinuous dependencies, long-distance movement, and scrambler constructs, which are central to modeling free word‑order languages.</p> </li> <li> <p>There’s no discussion of deeper categorization: feature structures for number, case, animacy, tense/mood/aspect, or the role of verbal agreement outside the auxiliary.</p> </li> </ul> <p><br/> <br/> <br/> <br/></p> <h2 id="b-grammatical-theory-framework-system">B. Grammatical Theory/ Framework/ System</h2> <hr/> <p>This paper does not clearly follow any of the major grammatical theories— at least not in a consistent or principled way. By grammatical theories, I mean theories like- Traditional Grammar (TG), Structural Grammar (SG), Transformational-Generative Grammar (TGG), Functional Grammar (FG), Dependency Grammar (DG), Lexical Functional Grammar (LFG), Head-Driven Phrase Structure Grammar (HPSG), Construction Grammar (CG) etc.</p> <table> <thead> <tr> <th>Grammatical Theory</th> <th>Does Paper Follow It?</th> <th>Why / Why Not</th> </tr> </thead> <tbody> <tr> <td>Traditional Grammar (TG)</td> <td>❌</td> <td>Not descriptive or terminology-based; author aims for formal rules.</td> </tr> <tr> <td>Structural Grammar (SG)</td> <td>❌</td> <td>Author has phrase rules, but SG focuses on distributions, not formal parsing.</td> </tr> <tr> <td>Transformational-Generative Grammar (TGG)</td> <td>❌</td> <td>No transformations (movement, deletion, etc.), no deep/surface structure.</td> </tr> <tr> <td>Functional Grammar (FG)</td> <td>❌</td> <td>No thematic roles, no discourse function labels, no functional motivation.</td> </tr> <tr> <td>Dependency Grammar (DG)</td> <td>❌</td> <td>No head-dependent relations, no dependency trees.</td> </tr> <tr> <td>Lexical Functional Grammar (LFG)</td> <td>❌</td> <td>No f-structures or c-structures, no mapping from lexical entries to syntax.</td> </tr> <tr> <td>Head-Driven Phrase Structure Grammar (HPSG)</td> <td>❌</td> <td>No feature structures, no unification, no typed logic.</td> </tr> <tr> <td>Construction Grammar (CG)</td> <td>❌</td> <td>No form-meaning pairings or construction-level generalizations.</td> </tr> </tbody> </table> <p><br/> <br/> <br/> This paper attempted a simplified CFG-based analysis, loosely reminiscent of early Context-Free Phrase Structure Grammars as used in early NLP (e.g., before the statistical revolution). If author wants to improve this work and make it publishable in a stronger venue, choosing one coherent grammatical theory and sticking with it will greatly help. Some suggestions:</p> <p><br/></p> <h6 id="option-1-dependency-grammar-ud-style"><strong>Option 1: Dependency Grammar (UD-style)</strong></h6> <p>If author is interested in parser implementation and wish to move toward NLP applications, DG is a better choice. It’s flexible and aligns with existing corpora (e.g., Hindi UD Treebank). Use head-modifier dependencies, model relations like <code class="language-plaintext highlighter-rouge">nsubj</code>, <code class="language-plaintext highlighter-rouge">obj</code>, <code class="language-plaintext highlighter-rouge">aux</code>, and handle scrambling via <code class="language-plaintext highlighter-rouge">edge labels</code>.</p> <p><br/></p> <h6 id="option-2-lexical-functional-grammar-lfg"><strong>Option 2: Lexical Functional Grammar (LFG)</strong></h6> <p>LFG is good for free word order languages like Hindi. Allows <a href="">c-structure and f-structure</a> separation. You can model:</p> <ul> <li>Surface word order using phrase structure rules (c-structure)</li> <li>Underlying relations (subject, object) using functional structures (f-structure)</li> <li>Gender, number, case as feature attributes</li> </ul> <p><br/></p> <h6 id="option-3-hpsg-head-driven-phrase-structure-grammar"><strong>Option 3: HPSG (Head-Driven Phrase Structure Grammar)</strong></h6> <p>HPSG is also great for morphologically rich languages. Rich in typed feature structures and lexical specifications. It is excellent for: Agreement modelling, Subject/object marking and Constraint-based parsing.</p> <p><br/> <br/> <br/> <br/></p> <h2 id="summary-and-improvement">Summary and Improvement</h2> <hr/> <p>Some major points of improvement for this paper are:</p> <ul> <li> <p>Clarify theoretical framework: state whether this is CFG-based Montague semantics or a hybrid, and defend that choice.</p> </li> <li> <p>Fully enumerate CFG rules (means listing all grammar rules in an ordered, numbered way, following formal notation.) in a consistent notation, including feature-attributes (gender, number, case, etc.).</p> </li> <li> <p>Expand grammar coverage to include optional orderings and scrambling.</p> </li> <li> <p>Formalize gender/agreement/negation using features or typed structures rather than ad hoc transformations.</p> </li> <li> <p>Provide corpus and evaluation metrics: size, domain, parsing accuracy, coverage, ambiguity rates.</p> </li> <li> <p>Publish code or grammar file for reproducibility and allow community verification.</p> </li> <li> <p>Compare against baselines: dependency parsers, formal grammar systems (GPSG, LFG, CG etc.).</p> </li> <li> <p>Address ambiguity management and discuss parse selection or ranking strategies.</p> </li> </ul>]]></content><author><name></name></author><category term="review-posts"/><category term="grammar"/><category term="syntax"/><category term="formalrules"/><summary type="html"><![CDATA[it remains more of a sketch than a robust theory]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://iamalinguist.github.io/blog/2025/understanding/" rel="alternate" type="text/html" title="a post with tabs"/><published>2025-06-07T00:32:13+00:00</published><updated>2025-06-07T00:32:13+00:00</updated><id>https://iamalinguist.github.io/blog/2025/understanding</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/understanding/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="6fd84a3a-3604-434f-8f4f-42e087d9e773" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="6fd84a3a-3604-434f-8f4f-42e087d9e773" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="1d12df2d-2ded-483a-a898-6531fa92f220" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="1d12df2d-2ded-483a-a898-6531fa92f220" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="32655650-dbc6-4f9d-b2c4-69e61f7ea6ea" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="32655650-dbc6-4f9d-b2c4-69e61f7ea6ea" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a note for ‘Linguistics + NLP’ domain</title><link href="https://iamalinguist.github.io/blog/2025/a-note-ling-nlp/" rel="alternate" type="text/html" title="a note for ‘Linguistics + NLP’ domain"/><published>2025-05-31T09:00:00+00:00</published><updated>2025-05-31T09:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/a-note-ling-nlp</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/a-note-ling-nlp/"><![CDATA[<p>While Linguistics seeks to provide the formal, theoretical, and descriptive tools to understand the human language, NLP seeks to computationally model it (for tasks like translation, parsing, sentiment analysis etc.).</p> <p>Combining Linguistics with NLP leads to <em>robust</em>, <em>explainable</em>, and <em>cross-linguistically aware</em> models. While modern deep learning approaches in NLP sometimes bypass linguistic theory, the long-term sustainability of language technologies—especially for low-resource, morphologically rich, or culturally embedded languages—depends on linguistic insight.</p> <p><br/></p> <blockquote> <p>This interdisciplinary journey is not just about building tools that “work”, but about building systems that “understand”.</p> </blockquote> <p><br/> This section outlines key domains/research areas where such integration is both necessary and promising.</p> <hr/> <p><br/> <br/></p> <h2 id="1-formal-grammars-in-nlp">1. Formal Grammars in NLP</h2> <p>At the research frontier, the syntactic structure of natural languages is no longer modeled with simple rules. Instead, linguistically informed <strong>formal grammars</strong> such as Tree-Adjoining Grammar (TAG), Combinatory Categorial Grammar (CCG), Head-driven Phrase Structure Grammar (HPSG), and Minimalist Grammars (MG) are employed. These frameworks capture long-distance dependencies, coordination, and movement more naturally than context-free grammars (CFGs).</p> <ul> <li><strong>TAG</strong>: Suitable for modeling recursion and crossing dependencies.</li> <li><strong>CCG</strong>: Offers transparent syntax-semantics mapping using combinatory logic.</li> <li><strong>MG</strong>: Grounded in Chomsky’s Minimalist Program, it provides a generative account of human language using operations like <em>Merge</em> and <em>Move</em>.</li> </ul> <p>Such grammars are essential for <em>syntactic parsing</em>, especially in linguistically complex or low-resource languages.</p> <hr/> <p><br/> <br/></p> <h2 id="2-compositional-semantics-and-lambda-calculus">2. Compositional Semantics and Lambda Calculus</h2> <p>Advanced semantic modeling involves <strong>compositionality</strong>—the principle that the meaning of a sentence is determined by its parts and their arrangement. This is formalized using <em>typed lambda calculus</em>, where:</p> <ul> <li>Noun phrases, verbs, and modifiers are treated as functions or arguments.</li> <li>Complex meanings are built incrementally by function application.</li> </ul> <p><em>Montague Semantics</em>, <em>Intensional Logic</em>, and <em>Dynamic Semantics</em> offer logical systems to model ambiguity, quantification, modality, and discourse reference. (These topics are widely new to me).</p> <hr/> <p><br/> <br/></p> <h2 id="3-abstract-meaning-representation-amr-and-graph-based-semantics">3. Abstract Meaning Representation (AMR) and Graph-Based Semantics</h2> <p>Abstract Meaning Representation (AMR) provides a graph-based formalism for sentence meaning. Unlike trees, AMR allows multiple incoming edges, reentrancy, and coreference, making it suitable for representing:</p> <ul> <li> <p>Entity relationships</p> </li> <li> <p>Events and arguments</p> </li> <li> <p>Quantification and scope</p> </li> <li> <p>Modality and negation</p> </li> </ul> <p><strong>AMR parsing</strong> is a key task in computational semantics and is central to information extraction, question answering, and knowledge integration.</p> <hr/> <p><br/> <br/></p> <h2 id="4-discourse-and-dynamic-semantics">4. Discourse and Dynamic Semantics</h2> <p>Modeling extended texts and dialogues requires going beyond sentence-level semantics:</p> <ul> <li> <p>Discourse Representation Theory (DRT) introduces intermediate structures for handling anaphora and temporal relations.</p> </li> <li> <p>Dynamic Semantics (like Dynamic Predicate Logic) allows meanings to update the “context state” as discourse progresses.</p> </li> </ul> <p>Such frameworks are crucial for coreference resolution, dialogue systems, and narrative understanding.</p> <hr/> <p><br/> <br/></p> <h2 id="5-multilinguality-and-linguistic-typology-in-nlp">5. Multilinguality and Linguistic Typology in NLP</h2> <p>Research-level NLP increasingly addresses the typological diversity of the world’s languages:</p> <ul> <li> <p>Projects like Universal Dependencies (UD) aim to standardize syntactic annotation across languages.</p> </li> <li> <p>Typological Databases (e.g., WALS, PHOIBLE) inform models about word order, morphology, and phonological inventories.</p> </li> </ul> <p>In multilingual NLP:</p> <ul> <li> <p>Transfer learning and multilingual LLMs (e.g., mBERT, XLM-R) help generalize across languages.</p> </li> <li> <p>Linguistically grounded models improve performance on low-resource languages by leveraging structural knowledge.</p> </li> </ul> <hr/> <p><br/> <br/></p> <h2 id="6-inference-world-knowledge-and-commonsense-reasoning">6. Inference, World Knowledge, and Commonsense Reasoning</h2> <p>Semantic processing increasingly intersects with AI reasoning:</p> <ul> <li> <p>Natural Logic and Entailment Systems evaluate valid inferences from language.</p> </li> <li> <p>Commonsense reasoning tasks (e.g., Winograd Schema, Story Cloze Test) demand real-world background knowledge.</p> </li> <li> <p>Ontologies like FrameNet, VerbNet, and ConceptNet provide structured knowledge for event semantics and role labeling.</p> </li> </ul> <p>Inference also plays a role in explainable NLP, where output must be justified with traceable logical steps.</p> <hr/> <p><br/> <br/></p> <h2 id="7-language-and-logic-type-theory-and-higher-order-semantics">7. Language and Logic: Type Theory and Higher-Order Semantics</h2> <p>Advanced systems often adopt Type Theory and Categorial Grammar to provide highly compositional, structured accounts of meaning. Examples include:</p> <ul> <li> <p>Montague Grammar: Uses types e (entity) and t (truth value) to build meaning recursively.</p> </li> <li> <p>λμ Calculus (Lambda Mu Calculus), Linear Logic, and Dependent Type Theory: Extend the expressive power of semantic models.</p> </li> </ul> <hr/> <p><br/> <br/></p> <p><strong>Conclusion</strong>: Advanced topics in Linguistics + NLP reflect the need for interpretable, generalizable, and cognitively aligned models. As language models grow in size and capability, their alignment with formal linguistic theories becomes both a challenge and an opportunity.</p> <p><br/> <br/></p> <h2 id="recommended-reading--resources">Recommended Reading &amp; Resources</h2> <ul> <li>Chomsky - <em>Syntactic Structures</em></li> <li>Jacob Eisenstein - <em>Natural Language Processing</em></li> <li><a href="https://aclanthology.org">ACL Anthology</a> - The Association for Computational Linguistics</li> </ul>]]></content><author><name></name></author><category term="comp-ling"/><category term="grammar"/><category term="nlp"/><summary type="html"><![CDATA[A Comprehensive Guide to Key Research Areas]]></summary></entry><entry><title type="html">a note for ‘Syntax + Hindi’ Study</title><link href="https://iamalinguist.github.io/blog/2025/a-note-syn-hin/" rel="alternate" type="text/html" title="a note for ‘Syntax + Hindi’ Study"/><published>2025-05-31T09:00:00+00:00</published><updated>2025-05-31T09:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/a-note-syn-hin</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/a-note-syn-hin/"><![CDATA[<p>Hindi, being a relatively underexplored language in formal syntactic theory, presents rich opportunities to test, refine, or reformulate syntactic assumptions. The intersection of <a href="https://iamalinguist.github.io/blog/2025/a-note-ling-syn/">Syntax</a> and Hindi is a fertile ground for both theoretical innovation and empirical depth. Here are emerging or underexploited areas of research that can guide seminars and research in the coming five years:</p> <p><br/></p> <h4 id="1-scrambling-and-word-order-flexibility">1. Scrambling and Word Order Flexibility</h4> <p>Hindi’s relatively free word order, driven by scrambling, offers a natural laboratory to explore syntactic operations beyond English SVO rigidity. Key questions involve:</p> <ul> <li>What are the constraints on scrambling?</li> <li>Does scrambling have interpretive effects (e.g., focus, topic, contrast)?</li> <li>Can Hindi scrambling challenge minimalist assumptions about movement?</li> </ul> <p><br/></p> <h4 id="2-complex-predicates-and-light-verbs">2. Complex Predicates and Light Verbs</h4> <p>Hindi extensively uses light verb constructions (e.g., <em>khā liyā</em>, <em>likh diyā</em>). These can be used to explore:</p> <ul> <li>Argument structure variation</li> <li>Verb serialization and compositional semantics</li> <li>Syntactic projection of aspectual and agentive features</li> </ul> <p><br/></p> <h4 id="3-differential-object-marking-dom">3. Differential Object Marking (DOM)</h4> <p>The optional accusative marker <em>-ko</em> in Hindi offers rich data for:</p> <ul> <li>The syntax of case marking</li> <li>Animacy and definiteness effects</li> <li>Typological parallels and theoretical challenges to Case Theory</li> </ul> <div style="background-color: #f0f0f0; padding: 10px; border-left: 4px solid #007bff;"> <strong>Note:</strong> In DOM, the term "differential" refers to the phenomenon where not all direct objects in a language are marked in the same way—some receive special morphological marking (like the Hindi postposition -ko) based on certain features of the object. In Hindi, for example: <ul> <li>Maine ek kitāb paṛhī. (I read a book.) → kitāb is unmarked.</li> <li>Maine Rām-ko dekha. (I saw Ram.) → Rām is marked with -ko.</li> </ul> The difference (hence “differential”) depends on properties such as: <ul> <li>Animacy: animate objects are more likely to be marked.</li> <li>Definiteness: definite/specific objects tend to be marked.</li> <li>Topicality or affectedness: marked if the object is emphasized or topical.</li> </ul> <br/> So, “differential” captures the idea that *object marking is not uniform* —it varies *systematically* depending on semantic or pragmatic features. </div> <p><br/></p> <h4 id="4-agreement-patterns-and-feature-geometry">4. Agreement Patterns and Feature Geometry</h4> <p>Hindi displays interesting agreement asymmetries, especially with ergative constructions and object agreement in perfective clauses:</p> <ul> <li>What does Hindi agreement reveal about the directionality of Agree?</li> <li>Can Hindi challenge standard assumptions about phi-features?</li> </ul> <p><br/></p> <h4 id="5-wh-questions-and-clause-typing">5. Wh-questions and Clause Typing</h4> <p>Hindi employs sentence-final wh-elements and lacks overt complementizers in many embedded clauses:</p> <ul> <li>What syntactic mechanisms allow clause-typing?</li> <li>Can Hindi offer a different model for interrogative clause formation?</li> </ul> <p><br/></p> <h4 id="6-negation-modality-and-polarity-items">6. Negation, Modality, and Polarity Items</h4> <p>The interaction between negation (<em>nahī̃</em>), polarity-sensitive items, and modals in Hindi has implications for:</p> <ul> <li>Syntax-semantics interface</li> <li>Scope ambiguity</li> <li>Licensing conditions for NPIs and their syntactic positions</li> </ul> <p><br/></p> <h4 id="7-relative-clauses-and-correlatives">7. Relative Clauses and Correlatives</h4> <p>Hindi uses correlative structures (e.g., <em>jo…vo…</em>) rather than internally headed relative clauses:</p> <ul> <li>What does this mean for head-dependency relations?</li> <li>Can correlatives inform syntactic theories of left-periphery or CP structure?</li> </ul> <p><br/></p> <h4 id="8-syntax-of-evidentiality-and-reported-speech">8. Syntax of Evidentiality and Reported Speech</h4> <p>Hindi uses lexical items (e.g., <em>kya</em>, <em>to</em>, <em>lagta hai</em>) to express epistemic status and source of information. Studying this domain syntactically and semantically can enrich:</p> <ul> <li>The theory of propositional attitude verbs</li> <li>The role of syntax in evidential interpretation</li> </ul> <p><br/></p> <h4 id="9-tense-aspect-systems-and-perfectivity">9. Tense-Aspect Systems and Perfectivity</h4> <p>Hindi’s split ergativity and aspect-driven agreement offer critical data for:</p> <ul> <li>Temporal anchoring in clause structure</li> <li>Aspectual auxiliary syntax</li> <li>Interface of morphology and syntax</li> </ul> <p><br/></p> <h4 id="10-formal-grammar-of-hindi-lfg-hpsg-ccg-minimalism">10. Formal Grammar of Hindi (LFG, HPSG, CCG, Minimalism)</h4> <p>Developing formally annotated treebanks and rule sets for Hindi using modern grammar formalisms will contribute to both linguistic theory and computational modeling. <br/> <br/> <br/></p> <hr/> <h3 id="conclusion-a-five-year-research-vision">Conclusion: A Five-Year Research Vision</h3> <p>I welcome scholars and researchers interested in collaborating on these ten topics. This work will adhere strictly to the outlined ideas and will further involve:</p> <ul> <li><strong>Organizing thematic seminars and workshops</strong> to explore these topics in depth.</li> <li><strong>Developing formal grammars of Hindi</strong>, incorporating syntactic frameworks such as Minimalism, LFG, or CCG.</li> <li><strong>Contributing to the creation of computational resources</strong> that enhance Hindi syntax analysis.</li> <li><strong>Examining and, if necessary, challenging Eurocentric syntactic models</strong> by presenting data-driven modifications based on Hindi.</li> </ul> <p>I look forward to meaningful collaborations that advance research in these areas.</p> <hr/>]]></content><author><name></name></author><category term="theo-linguistics"/><category term="hi-linguistics"/><category term="grammar"/><category term="syntax"/><summary type="html"><![CDATA[A Comprehensive Guide to Core Topics in Hindi Syntactic Research]]></summary></entry><entry><title type="html">a note for ‘Syntax’ Study</title><link href="https://iamalinguist.github.io/blog/2025/a-note-syn/" rel="alternate" type="text/html" title="a note for ‘Syntax’ Study"/><published>2025-05-31T09:00:00+00:00</published><updated>2025-05-31T09:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2025/a-note-syn</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2025/a-note-syn/"><![CDATA[<p>Syntax examines the <strong>rules and principles that govern the structure of sentences</strong> in natural languages. Over the years, syntactic research has significantly contributed to our understanding of language universals and cross-linguistic variation. This article discusses key research themes in the field of syntax (first part) and highlights a specific area of interest for syntactic studies in Hindi (second part):</p> <p><br/> <br/></p> <h4 id="1-constituent-structure-and-phrase-structure-grammar">1. Constituent Structure and Phrase Structure Grammar</h4> <p>Understanding how words group into phrases and how phrases build hierarchical syntactic structures (such as NPs, VPs, CPs) remains crucial. X-bar theory and later, minimalist projections, help formalize such structures.</p> <p><br/></p> <h4 id="2-the-minimalist-program">2. The Minimalist Program</h4> <p>Noam Chomsky’s Minimalist Program is a dominant research framework that aims to explain syntactic phenomena through economy principles and universal computational mechanisms. Key topics include Merge, Move, Agree, and Feature Checking.</p> <p><br/></p> <h4 id="3-syntactic-movement-and-constraints">3. Syntactic Movement and Constraints</h4> <p>Topics such as Wh-movement, topicalization, scrambling, and relativization are extensively studied, especially in terms of constraints like Subjacency, Island Conditions, and Locality.</p> <p><br/></p> <h4 id="4-case-theory-and-theta-theory">4. Case Theory and Theta Theory</h4> <p>How arguments receive structural or inherent Case and how predicates assign thematic roles (Agent, Theme, Goal) remain essential to syntactic analysis, especially when dealing with predicate-argument structures.</p> <p><br/></p> <h4 id="5-binding-theory">5. Binding Theory</h4> <p>The study of anaphora, coreference, and pronoun resolution through Principles A, B, and C of the Binding Theory is still relevant in empirical and theoretical work.</p> <p><br/></p> <h4 id="6-syntax-semantics-interface">6. Syntax-Semantics Interface</h4> <p>Exploring how syntactic structures map onto semantic interpretations, especially in relation to scope, quantification, and tense-aspect systems. Topics like event semantics, type-theoretic composition, and intensionality are commonly discussed here.</p> <p><br/></p> <h4 id="7-syntactic-typology-and-cross-linguistic-variation">7. Syntactic Typology and Cross-Linguistic Variation</h4> <p>Comparative syntax explores how syntactic features (like word order, agreement, null subject phenomena) vary across languages, enriching typological databases and theoretical generalizations.</p> <p><br/></p> <h4 id="8-syntactic-representation-in-language-acquisition">8. Syntactic Representation in Language Acquisition</h4> <p>Topics like how syntactic structures are learned, processed in real-time, or represented in the brain form the bridge between syntax and psycholinguistics.</p> <hr/> <h3 id="recommended-reading--resources">Recommended Reading &amp; Resources</h3> <blockquote> <p>Texts 1, 2, and 3 are introductory; 4, 5, and 6 are intermediate; and 7, 8, and 9 are advanced.</p> </blockquote> <ol> <li> <p>Carnie, Andrew. Syntax: A Generative Introduction → Covers phrase structure, X-bar theory, movement, features, etc.</p> </li> <li> <p>Tallerman, Maggie. Understanding Syntax → Great for typological diversity and beginner-friendly explanations.</p> </li> <li> <p>Radford, Andrew. Syntax: A Minimalist Introduction → A simplified path into Chomsky’s Minimalist Program.</p> </li> <li> <p>Koeneman &amp; Zeijlstra. Introducing Syntax → Well-structured, with good exercises and real-language data.</p> </li> <li> <p>Sportiche, Koopman, and Stabler. An Introduction to Syntactic Analysis and Theory → Slightly formal, ideal for those with a background in logic or formal linguistics.</p> </li> <li> <p>Haegeman, Liliane. Introduction to Government and Binding Theory → A classical GB-theory book, useful for understanding the foundations.</p> </li> <li> <p>Chomsky, Noam. The Minimalist Program → Foundational for theoretical syntax in current generative grammar.</p> </li> <li> <p>Adger, David. Core Syntax → Very formal and structured.</p> </li> <li> <p>Stabler, Edward. Papers on Minimalist Grammars and Parsing → Good for connecting formal syntax to computational models.</p> </li> </ol>]]></content><author><name></name></author><category term="theo-linguistics"/><category term="grammar"/><category term="syntax"/><summary type="html"><![CDATA[A Comprehensive Guide to Key Areas of Syntactic Research]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://iamalinguist.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://iamalinguist.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://iamalinguist.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://iamalinguist.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://iamalinguist.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>